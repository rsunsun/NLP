{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aaee3e6a07264f0c87493efacb5cce54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c924330425c14cfda6e9964b8c16a2e7",
              "IPY_MODEL_71828f10bc4c4b0cbc826721905012f0",
              "IPY_MODEL_bfd7a54d95134f788be6ed34cc68fae1"
            ],
            "layout": "IPY_MODEL_1ed61472150d4689901e8e39ebdeaf36"
          }
        },
        "c924330425c14cfda6e9964b8c16a2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4a024003224ed2a8dd75684d8a2e43",
            "placeholder": "​",
            "style": "IPY_MODEL_2eb3759984494068935fd1446391dd21",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "71828f10bc4c4b0cbc826721905012f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66454662f184fff8607959086791a57",
            "max": 689,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f98f3d6de78e41f58fd3a3573fc855cc",
            "value": 689
          }
        },
        "bfd7a54d95134f788be6ed34cc68fae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b827a101863f43d2bb16f0cf5655fd8b",
            "placeholder": "​",
            "style": "IPY_MODEL_4b1977f1b4b742f2a8471e62ec932709",
            "value": " 689/689 [00:00&lt;00:00, 36.6kB/s]"
          }
        },
        "1ed61472150d4689901e8e39ebdeaf36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4a024003224ed2a8dd75684d8a2e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb3759984494068935fd1446391dd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66454662f184fff8607959086791a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f98f3d6de78e41f58fd3a3573fc855cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b827a101863f43d2bb16f0cf5655fd8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1977f1b4b742f2a8471e62ec932709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b1dd2c764044dcea2f4c70cb1437014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c5a1c0779e14127ad9330415f61a1ab",
              "IPY_MODEL_ef7c56afabbc484dbdb303f2cbdb2462",
              "IPY_MODEL_b8150d47dc3c44e3ac0cc690c6e30119"
            ],
            "layout": "IPY_MODEL_9c938c8dfcf74b5dbaf10904b7fa5a55"
          }
        },
        "1c5a1c0779e14127ad9330415f61a1ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d85f9ce6434d8a849d1a1a72ed9797",
            "placeholder": "​",
            "style": "IPY_MODEL_0fd9893ad16542ce8dcd7c2d3e3bf760",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "ef7c56afabbc484dbdb303f2cbdb2462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e154a16770a4e91bc70292d0b4b4bb0",
            "max": 6431878936,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79a0fa75118048d3b6475beb297a1b08",
            "value": 6431878936
          }
        },
        "b8150d47dc3c44e3ac0cc690c6e30119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b155db82d1f44befa5d214a24d67f960",
            "placeholder": "​",
            "style": "IPY_MODEL_6ed8b71213924efd861d2ab1d5942fad",
            "value": " 6.43G/6.43G [00:29&lt;00:00, 408MB/s]"
          }
        },
        "9c938c8dfcf74b5dbaf10904b7fa5a55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d85f9ce6434d8a849d1a1a72ed9797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fd9893ad16542ce8dcd7c2d3e3bf760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e154a16770a4e91bc70292d0b4b4bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79a0fa75118048d3b6475beb297a1b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b155db82d1f44befa5d214a24d67f960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ed8b71213924efd861d2ab1d5942fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "638a7ada7ecd4108b5bf58ba958baef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e52ce19d3e5d44a5894e0c5f602e24a9",
              "IPY_MODEL_2ffe9a1c7ec94776a351edec3dfa05f2",
              "IPY_MODEL_b73dfdc6c7f8405f9dbadae090441465"
            ],
            "layout": "IPY_MODEL_47af6a6377524fc5b99ce65257690afa"
          }
        },
        "e52ce19d3e5d44a5894e0c5f602e24a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c48f07d2633442ca7a50eb7dbefcacd",
            "placeholder": "​",
            "style": "IPY_MODEL_ac0797f03e6143e190a3e9765103c248",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "2ffe9a1c7ec94776a351edec3dfa05f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69862f58211b4b09b3f0b0cf00b3a01b",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60ff20c9a6c54bcc87a90a0b4ba8998b",
            "value": 124
          }
        },
        "b73dfdc6c7f8405f9dbadae090441465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e35cd8f6e0ff487d89f3363eb208d8b4",
            "placeholder": "​",
            "style": "IPY_MODEL_3c8b63aaeef64f709bc57583c8a2472b",
            "value": " 124/124 [00:00&lt;00:00, 6.73kB/s]"
          }
        },
        "47af6a6377524fc5b99ce65257690afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c48f07d2633442ca7a50eb7dbefcacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0797f03e6143e190a3e9765103c248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69862f58211b4b09b3f0b0cf00b3a01b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ff20c9a6c54bcc87a90a0b4ba8998b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e35cd8f6e0ff487d89f3363eb208d8b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c8b63aaeef64f709bc57583c8a2472b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#nanoGPT 실행 및 활용\n",
        "1. shakespeare 작품 dataset으로 GPT train 및 generation \n",
        "2. finetuning하여 gpt2-xl train 및 generation \n",
        "3. shakespeare 프로그램 변형하여 songwriter dataset으로 GPT train 및 generation \n",
        "\n",
        "프로그램 환경 : ColabPro+"
      ],
      "metadata": {
        "id": "iGS9UoA-5GqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Install**\n",
        "transformers : load GPT-2 checkpoints를 위해 huggingface transformers  install\n",
        "\n",
        "datasets :  OpenWebText를 다운로드하고 전처리(preprocess) 할때 필요\n",
        "\n",
        "tiktoken : OpenAI's fast BPE code 사용\n",
        "\n",
        "wandb : optional logging"
      ],
      "metadata": {
        "id": "jtUG41sJiM0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fvn8SoGh-gm",
        "outputId": "e0b86944-82f1-4788-f5cc-1c5586d780c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.20.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=8ed06f07561af0628d5415949b6b866d254c3738880a6796e4843f8ee128bf5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.20.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install tiktoken\n",
        "!pip install wandb\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "ALfTUBiTkE_g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### github에서 다운로드 한 nanoGPT 소스 파일 cloudServer에 load"
      ],
      "metadata": {
        "id": "xE6v3inDQh7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the name of the folder to download images\n",
        "img_folder_nm = 'nanoGPT'\n",
        "\n",
        "# set the name of the zip file to be downloaded\n",
        "img_zipfile_nm = 'nanoGPT.zip'\n",
        "\n",
        "# dropbox link for images\n",
        "dropbox_link = 'https://www.dropbox.com/sh/vaewl01pa8ewf26/AABKn6u0CDSn5JG3EHd6nvPXa?dl=0'"
      ],
      "metadata": {
        "id": "TcNPodGIiZqS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O {img_zipfile_nm} {dropbox_link}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aoj1DdpqiceR",
        "outputId": "9b1cab7a-4f06-4b00-9805-3e34e49c3b14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-24 01:14:52--  https://www.dropbox.com/sh/vaewl01pa8ewf26/AABKn6u0CDSn5JG3EHd6nvPXa?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /sh/raw/vaewl01pa8ewf26/AABKn6u0CDSn5JG3EHd6nvPXa [following]\n",
            "--2023-04-24 01:14:53--  https://www.dropbox.com/sh/raw/vaewl01pa8ewf26/AABKn6u0CDSn5JG3EHd6nvPXa\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1e46be0ebdd5c19d1bc419a08d.dl.dropboxusercontent.com/zip_download_get/BeJONjmQw03UjIYShn4PXvmlZQbAwTBsuYM19LLld-BayBfdNIwkcQ-HjU0SKm5VRw3HyMx8xsupKqRb1piOcmdFBN9OICf3pzCVo_c8-e1dmA# [following]\n",
            "--2023-04-24 01:14:54--  https://uc1e46be0ebdd5c19d1bc419a08d.dl.dropboxusercontent.com/zip_download_get/BeJONjmQw03UjIYShn4PXvmlZQbAwTBsuYM19LLld-BayBfdNIwkcQ-HjU0SKm5VRw3HyMx8xsupKqRb1piOcmdFBN9OICf3pzCVo_c8-e1dmA\n",
            "Resolving uc1e46be0ebdd5c19d1bc419a08d.dl.dropboxusercontent.com (uc1e46be0ebdd5c19d1bc419a08d.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6023:15::a27d:430f\n",
            "Connecting to uc1e46be0ebdd5c19d1bc419a08d.dl.dropboxusercontent.com (uc1e46be0ebdd5c19d1bc419a08d.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75451856 (72M) [application/zip]\n",
            "Saving to: ‘nanoGPT.zip’\n",
            "\n",
            "nanoGPT.zip         100%[===================>]  71.96M  13.6MB/s    in 6.0s    \n",
            "\n",
            "2023-04-24 01:15:00 (12.0 MB/s) - ‘nanoGPT.zip’ saved [75451856/75451856]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdY732X_iflV",
        "outputId": "96618b0c-3522-4435-9461-ff56df413f82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nanoGPT.zip  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!unzip {img_zipfile_nm} -d {img_folder_nm}"
      ],
      "metadata": {
        "id": "XU8t3B4kiiVq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {img_folder_nm}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUY8WLjzikeN",
        "outputId": "010f6d37-1931-4a7e-e9ea-529c1774fa15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t  configurator.py  model.py   scaling_laws.ipynb\n",
            "bench.py  data\t\t   README.md  train.py\n",
            "config\t  LICENSE\t   sample.py  transformer_sizing.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**quick start**\n",
        "\n",
        "## prepare.py  : \n",
        "Shakespeare 작품을 가지고 데이터 준비\n",
        "1. data load : Shakespeare 작품을 가지고 character-level GPT를 훈련\n",
        "2. 다운로드 shakespeare dataset : 데이터 세트 문자 길이  1,115,394\n",
        "3. 모든 고유 문자 갯수 : 65\n",
        "4. train과 test data set 분리 : \n",
        "5. train과 test data tokenize :  train has 1,003,854 tokens / val has 111,540 tokens\n",
        "\n",
        "  train_ids = encode(train_data)\n",
        "\n",
        "  val_ids = encode(val_data)\n",
        "\n",
        "5. train과 test data Token bin 파일 생성 :  to help us encode/decode:  train.bin, val.bin 파일\n",
        "6. encode/decode를 위해 활용할 meta 파일 생성성"
      ],
      "metadata": {
        "id": "98DnHrzgQ-K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/nanoGPT/data/shakespeare_char/prepare.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wUT70UBisWf",
        "outputId": "96353e0a-9bb4-477b-faa0-19fcf0270b25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 위 prepare.py 실행 후 train.bin, val.bin 파일이 생성됨을 확인 할 수 있다."
      ],
      "metadata": {
        "id": "mhemf11bRLRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/nanoGPT/data/shakespeare_char/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lszd1y3JizGh",
        "outputId": "726ad5f1-632f-45de-8e45-45d776752d8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.txt  meta.pkl  prepare.py  readme.md  train.bin  val.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('nanoGPT/')"
      ],
      "metadata": {
        "id": "q2F8td-ikACB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py : \n",
        "config/train_shakespeare_char.py config 파일을 이용해 GPT train\n",
        "\n",
        "1. train 할 gpt2의 config values setting \n",
        "\n",
        "  [data] dataset = 'shakespeare_char' / \n",
        "batch_size = 64  / \n",
        "block_size = 256\n",
        "\n",
        "  [GPT model] \n",
        "n_layer = 6 /\n",
        "n_head = 6 /\n",
        "n_embd = 384 /\n",
        "dropout = 0.2  \n",
        "\n",
        "  [learning rate decay settings] learning_rate = 1e-3  # max learning rate / \n",
        "max_iters = 5000  # total number of training iterations   /\n",
        "lr_decay_iters = 5000  /\n",
        "min_lr = 1e-4    # minimum learning rate\n",
        "\n",
        "2. GPT train을 위해 지정된 configurator.py을 불러옴 (train_shakespeare_char.py)\n",
        "\n",
        "  config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "\n",
        "  exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "\n",
        "  config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "\n",
        "3. various inits, derived attributes, I/O setup : \n",
        "total number of tokens per iteration (batch_size * block_size * radient_accumulation_steps ) : 655360\n",
        "\n",
        "4. model init\n",
        "\n",
        "    gptconf = GPTConfig(**model_args) : vocab_size = 65 \n",
        "\n",
        "    model = GPT(gptconf)\n",
        "\n",
        "5. compile the model\n",
        "\n",
        "  training loop :  \n",
        "  5.1 determine and set the learning rate for this iteration \n",
        "  \n",
        "  5.2 evaluate the loss on train/val sets and write checkpoints\n",
        "\n",
        "6.iteration 5000 수행 후 최적의  train loss / val loss : step 5000: train loss 0.0715, val loss 3.7191"
      ],
      "metadata": {
        "id": "w4Fy9lD-TBmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run train.py config/train_shakespeare_char.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7tmY0lSk6tU",
        "outputId": "d6727c95-4cef-4c4d-b547-be0bfb452629"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "total number of tokens per iteration: 655360\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-04-24 01:16:20,546] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:21,171] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:22,148] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:22,494] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:22,961] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:23,325] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:23,794] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:24,177] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:24,651] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:25,235] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:25,712] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-04-24 01:16:26,065] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: loss 4.2616, time 27805.33ms, mfu -100.00%\n",
            "iter 10: loss 3.2220, time 940.45ms, mfu 15.85%\n",
            "iter 20: loss 2.7700, time 927.58ms, mfu 15.87%\n",
            "iter 30: loss 2.6168, time 922.60ms, mfu 15.90%\n",
            "iter 40: loss 2.5380, time 942.05ms, mfu 15.89%\n",
            "iter 50: loss 2.5132, time 951.66ms, mfu 15.87%\n",
            "iter 60: loss 2.4730, time 939.12ms, mfu 15.87%\n",
            "iter 70: loss 2.4802, time 924.66ms, mfu 15.89%\n",
            "iter 80: loss 2.4408, time 961.47ms, mfu 15.85%\n",
            "iter 90: loss 2.4262, time 934.52ms, mfu 15.86%\n",
            "iter 100: loss 2.4047, time 929.98ms, mfu 15.88%\n",
            "iter 110: loss 2.3830, time 958.49ms, mfu 15.85%\n",
            "iter 120: loss 2.3723, time 929.44ms, mfu 15.87%\n",
            "iter 130: loss 2.3096, time 934.14ms, mfu 15.88%\n",
            "iter 140: loss 2.2582, time 947.57ms, mfu 15.86%\n",
            "iter 150: loss 2.1338, time 935.40ms, mfu 15.87%\n",
            "iter 160: loss 2.0861, time 931.74ms, mfu 15.88%\n",
            "iter 170: loss 2.0113, time 933.80ms, mfu 15.89%\n",
            "iter 180: loss 1.9555, time 975.49ms, mfu 15.83%\n",
            "iter 190: loss 1.8550, time 923.75ms, mfu 15.86%\n",
            "iter 200: loss 1.8575, time 935.06ms, mfu 15.87%\n",
            "iter 210: loss 1.8357, time 947.56ms, mfu 15.85%\n",
            "iter 220: loss 1.7513, time 938.13ms, mfu 15.86%\n",
            "iter 230: loss 1.7278, time 937.68ms, mfu 15.86%\n",
            "iter 240: loss 1.6909, time 939.94ms, mfu 15.86%\n",
            "step 250: train loss 1.5750, val loss 1.7540\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 1.6384, time 7726.93ms, mfu 14.47%\n",
            "iter 260: loss 1.6308, time 939.23ms, mfu 14.61%\n",
            "iter 270: loss 1.6213, time 939.50ms, mfu 14.73%\n",
            "iter 280: loss 1.5728, time 932.11ms, mfu 14.86%\n",
            "iter 290: loss 1.5420, time 940.07ms, mfu 14.96%\n",
            "iter 300: loss 1.5588, time 940.87ms, mfu 15.05%\n",
            "iter 310: loss 1.5242, time 931.27ms, mfu 15.14%\n",
            "iter 320: loss 1.5197, time 933.18ms, mfu 15.23%\n",
            "iter 330: loss 1.5323, time 940.23ms, mfu 15.29%\n",
            "iter 340: loss 1.4758, time 938.98ms, mfu 15.35%\n",
            "iter 350: loss 1.4631, time 935.76ms, mfu 15.40%\n",
            "iter 360: loss 1.3993, time 929.47ms, mfu 15.47%\n",
            "iter 370: loss 1.4118, time 934.67ms, mfu 15.52%\n",
            "iter 380: loss 1.3699, time 931.27ms, mfu 15.56%\n",
            "iter 390: loss 1.3564, time 941.10ms, mfu 15.59%\n",
            "iter 400: loss 1.3281, time 949.33ms, mfu 15.60%\n",
            "iter 410: loss 1.3577, time 955.67ms, mfu 15.60%\n",
            "iter 420: loss 1.3205, time 929.01ms, mfu 15.65%\n",
            "iter 430: loss 1.3102, time 932.21ms, mfu 15.68%\n",
            "iter 440: loss 1.3195, time 944.77ms, mfu 15.69%\n",
            "iter 450: loss 1.2939, time 939.72ms, mfu 15.71%\n",
            "iter 460: loss 1.2695, time 935.89ms, mfu 15.73%\n",
            "iter 470: loss 1.2817, time 950.27ms, mfu 15.72%\n",
            "iter 480: loss 1.2716, time 929.98ms, mfu 15.76%\n",
            "iter 490: loss 1.2433, time 944.97ms, mfu 15.76%\n",
            "step 500: train loss 1.1365, val loss 1.4758\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.2486, time 4339.07ms, mfu 14.52%\n",
            "iter 510: loss 1.2226, time 958.63ms, mfu 14.63%\n",
            "iter 520: loss 1.1837, time 941.94ms, mfu 14.75%\n",
            "iter 530: loss 1.2048, time 933.89ms, mfu 14.87%\n",
            "iter 540: loss 1.1913, time 935.38ms, mfu 14.97%\n",
            "iter 550: loss 1.1847, time 939.61ms, mfu 15.06%\n",
            "iter 560: loss 1.1997, time 931.53ms, mfu 15.16%\n",
            "iter 570: loss 1.1491, time 936.34ms, mfu 15.23%\n",
            "iter 580: loss 1.1700, time 944.12ms, mfu 15.29%\n",
            "iter 590: loss 1.1380, time 941.99ms, mfu 15.34%\n",
            "iter 600: loss 1.1277, time 934.07ms, mfu 15.40%\n",
            "iter 610: loss 1.1043, time 947.14ms, mfu 15.44%\n",
            "iter 620: loss 1.1131, time 938.15ms, mfu 15.48%\n",
            "iter 630: loss 1.0642, time 941.48ms, mfu 15.52%\n",
            "iter 640: loss 1.1163, time 929.70ms, mfu 15.57%\n",
            "iter 650: loss 1.0939, time 938.19ms, mfu 15.60%\n",
            "iter 660: loss 1.0640, time 947.01ms, mfu 15.61%\n",
            "iter 670: loss 1.0535, time 939.44ms, mfu 15.64%\n",
            "iter 680: loss 1.0508, time 943.32ms, mfu 15.66%\n",
            "iter 690: loss 1.0389, time 929.11ms, mfu 15.69%\n",
            "iter 700: loss 1.0245, time 942.11ms, mfu 15.71%\n",
            "iter 710: loss 1.0264, time 939.83ms, mfu 15.72%\n",
            "iter 720: loss 0.9937, time 938.63ms, mfu 15.74%\n",
            "iter 730: loss 1.0037, time 936.84ms, mfu 15.75%\n",
            "iter 740: loss 0.9774, time 930.70ms, mfu 15.78%\n",
            "step 750: train loss 0.7833, val loss 1.5877\n",
            "iter 750: loss 0.9800, time 4054.18ms, mfu 14.57%\n",
            "iter 760: loss 0.9822, time 946.97ms, mfu 14.69%\n",
            "iter 770: loss 0.9420, time 930.25ms, mfu 14.82%\n",
            "iter 780: loss 0.9627, time 942.72ms, mfu 14.92%\n",
            "iter 790: loss 0.9169, time 942.57ms, mfu 15.01%\n",
            "iter 800: loss 0.9315, time 935.01ms, mfu 15.10%\n",
            "iter 810: loss 0.8740, time 934.43ms, mfu 15.19%\n",
            "iter 820: loss 0.8773, time 929.37ms, mfu 15.27%\n",
            "iter 830: loss 0.8643, time 938.54ms, mfu 15.33%\n",
            "iter 840: loss 0.8579, time 936.03ms, mfu 15.39%\n",
            "iter 850: loss 0.8804, time 923.99ms, mfu 15.47%\n",
            "iter 860: loss 0.8260, time 937.28ms, mfu 15.51%\n",
            "iter 870: loss 0.8325, time 930.64ms, mfu 15.56%\n",
            "iter 880: loss 0.8255, time 929.21ms, mfu 15.61%\n",
            "iter 890: loss 0.8048, time 949.04ms, mfu 15.62%\n",
            "iter 900: loss 0.7845, time 929.68ms, mfu 15.66%\n",
            "iter 910: loss 0.8159, time 932.30ms, mfu 15.69%\n",
            "iter 920: loss 0.7600, time 928.07ms, mfu 15.73%\n",
            "iter 930: loss 0.7651, time 932.94ms, mfu 15.75%\n",
            "iter 940: loss 0.7680, time 931.11ms, mfu 15.78%\n",
            "iter 950: loss 0.7549, time 934.74ms, mfu 15.80%\n",
            "iter 960: loss 0.7428, time 958.08ms, mfu 15.77%\n",
            "iter 970: loss 0.7388, time 922.47ms, mfu 15.81%\n",
            "iter 980: loss 0.7087, time 933.87ms, mfu 15.83%\n",
            "iter 990: loss 0.7147, time 939.37ms, mfu 15.83%\n",
            "step 1000: train loss 0.4046, val loss 1.8885\n",
            "iter 1000: loss 0.6963, time 4087.21ms, mfu 14.61%\n",
            "iter 1010: loss 0.6910, time 927.12ms, mfu 14.76%\n",
            "iter 1020: loss 0.6864, time 928.11ms, mfu 14.89%\n",
            "iter 1030: loss 0.6669, time 923.52ms, mfu 15.01%\n",
            "iter 1040: loss 0.6712, time 947.02ms, mfu 15.09%\n",
            "iter 1050: loss 0.6532, time 929.10ms, mfu 15.18%\n",
            "iter 1060: loss 0.6431, time 935.05ms, mfu 15.26%\n",
            "iter 1070: loss 0.6133, time 961.72ms, mfu 15.28%\n",
            "iter 1080: loss 0.6375, time 934.94ms, mfu 15.35%\n",
            "iter 1090: loss 0.6244, time 925.36ms, mfu 15.42%\n",
            "iter 1100: loss 0.6243, time 937.22ms, mfu 15.47%\n",
            "iter 1110: loss 0.6112, time 926.61ms, mfu 15.53%\n",
            "iter 1120: loss 0.5840, time 933.02ms, mfu 15.58%\n",
            "iter 1130: loss 0.5888, time 949.64ms, mfu 15.59%\n",
            "iter 1140: loss 0.5838, time 943.66ms, mfu 15.61%\n",
            "iter 1150: loss 0.5949, time 926.65ms, mfu 15.66%\n",
            "iter 1160: loss 0.5733, time 926.98ms, mfu 15.70%\n",
            "iter 1170: loss 0.5478, time 965.11ms, mfu 15.67%\n",
            "iter 1180: loss 0.5661, time 932.54ms, mfu 15.70%\n",
            "iter 1190: loss 0.5424, time 933.41ms, mfu 15.73%\n",
            "iter 1200: loss 0.5342, time 943.39ms, mfu 15.74%\n",
            "iter 1210: loss 0.5307, time 937.65ms, mfu 15.75%\n",
            "iter 1220: loss 0.5239, time 942.68ms, mfu 15.76%\n",
            "iter 1230: loss 0.5244, time 923.87ms, mfu 15.80%\n",
            "iter 1240: loss 0.5264, time 943.03ms, mfu 15.80%\n",
            "step 1250: train loss 0.1961, val loss 2.2485\n",
            "iter 1250: loss 0.5146, time 4047.04ms, mfu 14.59%\n",
            "iter 1260: loss 0.5079, time 931.59ms, mfu 14.73%\n",
            "iter 1270: loss 0.5158, time 932.47ms, mfu 14.85%\n",
            "iter 1280: loss 0.4972, time 944.40ms, mfu 14.95%\n",
            "iter 1290: loss 0.4985, time 924.87ms, mfu 15.06%\n",
            "iter 1300: loss 0.4858, time 937.40ms, mfu 15.15%\n",
            "iter 1310: loss 0.4892, time 943.35ms, mfu 15.21%\n",
            "iter 1320: loss 0.4678, time 928.02ms, mfu 15.30%\n",
            "iter 1330: loss 0.4856, time 936.59ms, mfu 15.36%\n",
            "iter 1340: loss 0.4733, time 931.00ms, mfu 15.42%\n",
            "iter 1350: loss 0.4571, time 938.56ms, mfu 15.47%\n",
            "iter 1360: loss 0.4271, time 927.77ms, mfu 15.53%\n",
            "iter 1370: loss 0.4525, time 924.01ms, mfu 15.59%\n",
            "iter 1380: loss 0.4584, time 940.19ms, mfu 15.62%\n",
            "iter 1390: loss 0.4439, time 928.63ms, mfu 15.66%\n",
            "iter 1400: loss 0.4558, time 934.62ms, mfu 15.69%\n",
            "iter 1410: loss 0.4469, time 949.12ms, mfu 15.69%\n",
            "iter 1420: loss 0.4302, time 935.78ms, mfu 15.71%\n",
            "iter 1430: loss 0.4177, time 942.39ms, mfu 15.72%\n",
            "iter 1440: loss 0.4355, time 928.94ms, mfu 15.76%\n",
            "iter 1450: loss 0.4159, time 935.24ms, mfu 15.77%\n",
            "iter 1460: loss 0.4068, time 923.82ms, mfu 15.81%\n",
            "iter 1470: loss 0.4089, time 924.20ms, mfu 15.84%\n",
            "iter 1480: loss 0.4060, time 942.44ms, mfu 15.84%\n",
            "iter 1490: loss 0.4061, time 933.59ms, mfu 15.85%\n",
            "step 1500: train loss 0.1241, val loss 2.5036\n",
            "iter 1500: loss 0.3968, time 4054.54ms, mfu 14.63%\n",
            "iter 1510: loss 0.3972, time 938.08ms, mfu 14.76%\n",
            "iter 1520: loss 0.4028, time 949.31ms, mfu 14.85%\n",
            "iter 1530: loss 0.3822, time 926.65ms, mfu 14.98%\n",
            "iter 1540: loss 0.3906, time 930.19ms, mfu 15.08%\n",
            "iter 1550: loss 0.3891, time 935.42ms, mfu 15.17%\n",
            "iter 1560: loss 0.3783, time 946.35ms, mfu 15.23%\n",
            "iter 1570: loss 0.3921, time 940.58ms, mfu 15.29%\n",
            "iter 1580: loss 0.3908, time 927.25ms, mfu 15.37%\n",
            "iter 1590: loss 0.3824, time 960.94ms, mfu 15.38%\n",
            "iter 1600: loss 0.3696, time 940.38ms, mfu 15.43%\n",
            "iter 1610: loss 0.3792, time 930.27ms, mfu 15.49%\n",
            "iter 1620: loss 0.3810, time 936.51ms, mfu 15.53%\n",
            "iter 1630: loss 0.3560, time 930.02ms, mfu 15.58%\n",
            "iter 1640: loss 0.3746, time 934.29ms, mfu 15.62%\n",
            "iter 1650: loss 0.3689, time 934.84ms, mfu 15.65%\n",
            "iter 1660: loss 0.3770, time 927.06ms, mfu 15.69%\n",
            "iter 1670: loss 0.3711, time 934.18ms, mfu 15.72%\n",
            "iter 1680: loss 0.3472, time 933.10ms, mfu 15.74%\n",
            "iter 1690: loss 0.3537, time 943.18ms, mfu 15.75%\n",
            "iter 1700: loss 0.3618, time 927.59ms, mfu 15.78%\n",
            "iter 1710: loss 0.3482, time 929.38ms, mfu 15.81%\n",
            "iter 1720: loss 0.3468, time 940.85ms, mfu 15.81%\n",
            "iter 1730: loss 0.3401, time 948.91ms, mfu 15.80%\n",
            "iter 1740: loss 0.3370, time 937.89ms, mfu 15.81%\n",
            "step 1750: train loss 0.1032, val loss 2.7058\n",
            "iter 1750: loss 0.3525, time 4033.42ms, mfu 14.60%\n",
            "iter 1760: loss 0.3395, time 943.16ms, mfu 14.72%\n",
            "iter 1770: loss 0.3426, time 934.96ms, mfu 14.84%\n",
            "iter 1780: loss 0.3382, time 926.45ms, mfu 14.97%\n",
            "iter 1790: loss 0.3406, time 935.02ms, mfu 15.06%\n",
            "iter 1800: loss 0.3192, time 948.41ms, mfu 15.13%\n",
            "iter 1810: loss 0.3217, time 930.43ms, mfu 15.22%\n",
            "iter 1820: loss 0.3149, time 967.69ms, mfu 15.24%\n",
            "iter 1830: loss 0.3201, time 944.25ms, mfu 15.29%\n",
            "iter 1840: loss 0.3201, time 942.38ms, mfu 15.34%\n",
            "iter 1850: loss 0.3145, time 930.88ms, mfu 15.41%\n",
            "iter 1860: loss 0.3122, time 942.65ms, mfu 15.45%\n",
            "iter 1870: loss 0.3258, time 952.16ms, mfu 15.47%\n",
            "iter 1880: loss 0.3206, time 928.41ms, mfu 15.53%\n",
            "iter 1890: loss 0.3223, time 967.45ms, mfu 15.52%\n",
            "iter 1900: loss 0.3056, time 958.69ms, mfu 15.52%\n",
            "iter 1910: loss 0.3444, time 945.80ms, mfu 15.54%\n",
            "iter 1920: loss 0.3178, time 935.75ms, mfu 15.58%\n",
            "iter 1930: loss 0.3373, time 939.40ms, mfu 15.61%\n",
            "iter 1940: loss 0.2985, time 957.99ms, mfu 15.61%\n",
            "iter 1950: loss 0.3049, time 926.92ms, mfu 15.65%\n",
            "iter 1960: loss 0.3108, time 936.26ms, mfu 15.68%\n",
            "iter 1970: loss 0.3002, time 967.94ms, mfu 15.65%\n",
            "iter 1980: loss 0.2960, time 943.35ms, mfu 15.67%\n",
            "iter 1990: loss 0.3038, time 935.44ms, mfu 15.69%\n",
            "step 2000: train loss 0.0931, val loss 2.8596\n",
            "iter 2000: loss 0.2966, time 4068.98ms, mfu 14.49%\n",
            "iter 2010: loss 0.2995, time 948.33ms, mfu 14.61%\n",
            "iter 2020: loss 0.2976, time 938.92ms, mfu 14.74%\n",
            "iter 2030: loss 0.2929, time 935.77ms, mfu 14.86%\n",
            "iter 2040: loss 0.2946, time 924.76ms, mfu 14.98%\n",
            "iter 2050: loss 0.2932, time 952.21ms, mfu 15.05%\n",
            "iter 2060: loss 0.3006, time 947.92ms, mfu 15.12%\n",
            "iter 2070: loss 0.2870, time 932.02ms, mfu 15.21%\n",
            "iter 2080: loss 0.2865, time 931.37ms, mfu 15.29%\n",
            "iter 2090: loss 0.2863, time 938.39ms, mfu 15.35%\n",
            "iter 2100: loss 0.3055, time 932.31ms, mfu 15.41%\n",
            "iter 2110: loss 0.2967, time 932.29ms, mfu 15.47%\n",
            "iter 2120: loss 0.2850, time 954.49ms, mfu 15.48%\n",
            "iter 2130: loss 0.2941, time 944.52ms, mfu 15.51%\n",
            "iter 2140: loss 0.2926, time 924.75ms, mfu 15.57%\n",
            "iter 2150: loss 0.2865, time 953.42ms, mfu 15.58%\n",
            "iter 2160: loss 0.2872, time 936.51ms, mfu 15.61%\n",
            "iter 2170: loss 0.2692, time 932.43ms, mfu 15.65%\n",
            "iter 2180: loss 0.2857, time 927.34ms, mfu 15.69%\n",
            "iter 2190: loss 0.2787, time 930.63ms, mfu 15.72%\n",
            "iter 2200: loss 0.2804, time 935.85ms, mfu 15.74%\n",
            "iter 2210: loss 0.2832, time 923.12ms, mfu 15.78%\n",
            "iter 2220: loss 0.2690, time 936.71ms, mfu 15.80%\n",
            "iter 2230: loss 0.2762, time 926.58ms, mfu 15.83%\n",
            "iter 2240: loss 0.2817, time 927.20ms, mfu 15.85%\n",
            "step 2250: train loss 0.0882, val loss 2.9853\n",
            "iter 2250: loss 0.2787, time 4045.41ms, mfu 14.63%\n",
            "iter 2260: loss 0.2757, time 975.72ms, mfu 14.70%\n",
            "iter 2270: loss 0.2652, time 926.81ms, mfu 14.84%\n",
            "iter 2280: loss 0.2780, time 926.42ms, mfu 14.96%\n",
            "iter 2290: loss 0.2747, time 947.21ms, mfu 15.04%\n",
            "iter 2300: loss 0.2750, time 922.41ms, mfu 15.15%\n",
            "iter 2310: loss 0.2638, time 936.60ms, mfu 15.23%\n",
            "iter 2320: loss 0.2758, time 933.58ms, mfu 15.30%\n",
            "iter 2330: loss 0.2654, time 947.38ms, mfu 15.34%\n",
            "iter 2340: loss 0.2725, time 938.17ms, mfu 15.40%\n",
            "iter 2350: loss 0.2559, time 927.98ms, mfu 15.47%\n",
            "iter 2360: loss 0.2646, time 985.88ms, mfu 15.43%\n",
            "iter 2370: loss 0.2711, time 931.25ms, mfu 15.49%\n",
            "iter 2380: loss 0.2505, time 932.30ms, mfu 15.54%\n",
            "iter 2390: loss 0.2699, time 959.92ms, mfu 15.54%\n",
            "iter 2400: loss 0.2685, time 948.26ms, mfu 15.55%\n",
            "iter 2410: loss 0.2497, time 932.60ms, mfu 15.60%\n",
            "iter 2420: loss 0.2470, time 930.11ms, mfu 15.64%\n",
            "iter 2430: loss 0.2572, time 938.31ms, mfu 15.66%\n",
            "iter 2440: loss 0.2567, time 930.12ms, mfu 15.70%\n",
            "iter 2450: loss 0.2474, time 934.20ms, mfu 15.73%\n",
            "iter 2460: loss 0.2521, time 945.00ms, mfu 15.73%\n",
            "iter 2470: loss 0.2551, time 930.76ms, mfu 15.76%\n",
            "iter 2480: loss 0.2570, time 943.08ms, mfu 15.76%\n",
            "iter 2490: loss 0.2578, time 931.11ms, mfu 15.79%\n",
            "step 2500: train loss 0.0833, val loss 3.1118\n",
            "iter 2500: loss 0.2591, time 4050.24ms, mfu 14.58%\n",
            "iter 2510: loss 0.2515, time 926.19ms, mfu 14.73%\n",
            "iter 2520: loss 0.2628, time 923.43ms, mfu 14.87%\n",
            "iter 2530: loss 0.2650, time 924.39ms, mfu 15.00%\n",
            "iter 2540: loss 0.2472, time 937.83ms, mfu 15.09%\n",
            "iter 2550: loss 0.2457, time 934.98ms, mfu 15.17%\n",
            "iter 2560: loss 0.2489, time 938.70ms, mfu 15.24%\n",
            "iter 2570: loss 0.2439, time 942.86ms, mfu 15.30%\n",
            "iter 2580: loss 0.2525, time 937.42ms, mfu 15.36%\n",
            "iter 2590: loss 0.2478, time 925.57ms, mfu 15.43%\n",
            "iter 2600: loss 0.2466, time 952.46ms, mfu 15.45%\n",
            "iter 2610: loss 0.2470, time 935.36ms, mfu 15.50%\n",
            "iter 2620: loss 0.2382, time 926.66ms, mfu 15.56%\n",
            "iter 2630: loss 0.2423, time 928.91ms, mfu 15.61%\n",
            "iter 2640: loss 0.2451, time 950.60ms, mfu 15.62%\n",
            "iter 2650: loss 0.2490, time 931.72ms, mfu 15.65%\n",
            "iter 2660: loss 0.2388, time 935.40ms, mfu 15.68%\n",
            "iter 2670: loss 0.2444, time 948.96ms, mfu 15.68%\n",
            "iter 2680: loss 0.2392, time 940.13ms, mfu 15.70%\n",
            "iter 2690: loss 0.2440, time 947.25ms, mfu 15.71%\n",
            "iter 2700: loss 0.2388, time 936.85ms, mfu 15.73%\n",
            "iter 2710: loss 0.2468, time 947.34ms, mfu 15.73%\n",
            "iter 2720: loss 0.2414, time 944.88ms, mfu 15.73%\n",
            "iter 2730: loss 0.2382, time 925.42ms, mfu 15.77%\n",
            "iter 2740: loss 0.2269, time 1004.31ms, mfu 15.68%\n",
            "step 2750: train loss 0.0815, val loss 3.2123\n",
            "iter 2750: loss 0.2263, time 4117.63ms, mfu 14.47%\n",
            "iter 2760: loss 0.2362, time 932.26ms, mfu 14.62%\n",
            "iter 2770: loss 0.2338, time 923.37ms, mfu 14.77%\n",
            "iter 2780: loss 0.2321, time 960.62ms, mfu 14.85%\n",
            "iter 2790: loss 0.2309, time 936.93ms, mfu 14.95%\n",
            "iter 2800: loss 0.2243, time 948.28ms, mfu 15.03%\n",
            "iter 2810: loss 0.2232, time 920.77ms, mfu 15.15%\n",
            "iter 2820: loss 0.2209, time 945.82ms, mfu 15.21%\n",
            "iter 2830: loss 0.2367, time 946.05ms, mfu 15.26%\n",
            "iter 2840: loss 0.2206, time 927.18ms, mfu 15.34%\n",
            "iter 2850: loss 0.2220, time 945.64ms, mfu 15.39%\n",
            "iter 2860: loss 0.2262, time 934.29ms, mfu 15.44%\n",
            "iter 2870: loss 0.2299, time 934.91ms, mfu 15.49%\n",
            "iter 2880: loss 0.2229, time 937.51ms, mfu 15.53%\n",
            "iter 2890: loss 0.2264, time 934.45ms, mfu 15.57%\n",
            "iter 2900: loss 0.2216, time 930.79ms, mfu 15.62%\n",
            "iter 2910: loss 0.2187, time 932.27ms, mfu 15.66%\n",
            "iter 2920: loss 0.2228, time 933.34ms, mfu 15.69%\n",
            "iter 2930: loss 0.2228, time 930.08ms, mfu 15.72%\n",
            "iter 2940: loss 0.2226, time 933.65ms, mfu 15.75%\n",
            "iter 2950: loss 0.2162, time 938.00ms, mfu 15.76%\n",
            "iter 2960: loss 0.2221, time 927.12ms, mfu 15.79%\n",
            "iter 2970: loss 0.2183, time 934.78ms, mfu 15.81%\n",
            "iter 2980: loss 0.2191, time 937.45ms, mfu 15.82%\n",
            "iter 2990: loss 0.2202, time 932.75ms, mfu 15.83%\n",
            "step 3000: train loss 0.0788, val loss 3.2988\n",
            "iter 3000: loss 0.2230, time 4030.74ms, mfu 14.62%\n",
            "iter 3010: loss 0.2113, time 935.84ms, mfu 14.75%\n",
            "iter 3020: loss 0.2231, time 934.56ms, mfu 14.87%\n",
            "iter 3030: loss 0.2232, time 942.40ms, mfu 14.96%\n",
            "iter 3040: loss 0.2209, time 943.68ms, mfu 15.05%\n",
            "iter 3050: loss 0.2181, time 951.51ms, mfu 15.11%\n",
            "iter 3060: loss 0.2187, time 955.17ms, mfu 15.16%\n",
            "iter 3070: loss 0.2285, time 932.50ms, mfu 15.24%\n",
            "iter 3080: loss 0.2236, time 943.75ms, mfu 15.30%\n",
            "iter 3090: loss 0.2116, time 939.09ms, mfu 15.35%\n",
            "iter 3100: loss 0.2089, time 943.37ms, mfu 15.40%\n",
            "iter 3110: loss 0.2157, time 941.05ms, mfu 15.44%\n",
            "iter 3120: loss 0.2118, time 940.71ms, mfu 15.48%\n",
            "iter 3130: loss 0.2059, time 943.59ms, mfu 15.51%\n",
            "iter 3140: loss 0.2141, time 938.12ms, mfu 15.55%\n",
            "iter 3150: loss 0.2144, time 942.93ms, mfu 15.58%\n",
            "iter 3160: loss 0.2082, time 938.80ms, mfu 15.61%\n",
            "iter 3170: loss 0.2075, time 942.11ms, mfu 15.63%\n",
            "iter 3180: loss 0.2078, time 935.77ms, mfu 15.66%\n",
            "iter 3190: loss 0.2191, time 930.21ms, mfu 15.69%\n",
            "iter 3200: loss 0.2141, time 945.56ms, mfu 15.70%\n",
            "iter 3210: loss 0.2091, time 933.44ms, mfu 15.73%\n",
            "iter 3220: loss 0.2028, time 942.33ms, mfu 15.74%\n",
            "iter 3230: loss 0.2016, time 949.80ms, mfu 15.73%\n",
            "iter 3240: loss 0.1999, time 929.46ms, mfu 15.76%\n",
            "step 3250: train loss 0.0774, val loss 3.3820\n",
            "iter 3250: loss 0.2128, time 4057.32ms, mfu 14.55%\n",
            "iter 3260: loss 0.2036, time 941.49ms, mfu 14.68%\n",
            "iter 3270: loss 0.2002, time 968.53ms, mfu 14.75%\n",
            "iter 3280: loss 0.2022, time 923.53ms, mfu 14.89%\n",
            "iter 3290: loss 0.2017, time 928.12ms, mfu 15.01%\n",
            "iter 3300: loss 0.2074, time 933.08ms, mfu 15.10%\n",
            "iter 3310: loss 0.2073, time 940.90ms, mfu 15.18%\n",
            "iter 3320: loss 0.2034, time 920.57ms, mfu 15.28%\n",
            "iter 3330: loss 0.2026, time 941.65ms, mfu 15.33%\n",
            "iter 3340: loss 0.2043, time 946.19ms, mfu 15.38%\n",
            "iter 3350: loss 0.2081, time 927.30ms, mfu 15.45%\n",
            "iter 3360: loss 0.2100, time 937.65ms, mfu 15.49%\n",
            "iter 3370: loss 0.2060, time 936.45ms, mfu 15.53%\n",
            "iter 3380: loss 0.2128, time 956.84ms, mfu 15.54%\n",
            "iter 3390: loss 0.2047, time 937.32ms, mfu 15.57%\n",
            "iter 3400: loss 0.1987, time 945.88ms, mfu 15.59%\n",
            "iter 3410: loss 0.2021, time 947.35ms, mfu 15.61%\n",
            "iter 3420: loss 0.2002, time 942.18ms, mfu 15.63%\n",
            "iter 3430: loss 0.1981, time 931.75ms, mfu 15.66%\n",
            "iter 3440: loss 0.2067, time 945.02ms, mfu 15.68%\n",
            "iter 3450: loss 0.1994, time 930.29ms, mfu 15.71%\n",
            "iter 3460: loss 0.1920, time 937.40ms, mfu 15.73%\n",
            "iter 3470: loss 0.1966, time 931.06ms, mfu 15.76%\n",
            "iter 3480: loss 0.2055, time 933.73ms, mfu 15.78%\n",
            "iter 3490: loss 0.2013, time 933.21ms, mfu 15.80%\n",
            "step 3500: train loss 0.0759, val loss 3.4496\n",
            "iter 3500: loss 0.2112, time 4047.34ms, mfu 14.59%\n",
            "iter 3510: loss 0.1957, time 946.14ms, mfu 14.70%\n",
            "iter 3520: loss 0.1980, time 957.25ms, mfu 14.79%\n",
            "iter 3530: loss 0.1984, time 929.99ms, mfu 14.91%\n",
            "iter 3540: loss 0.1929, time 936.34ms, mfu 15.01%\n",
            "iter 3550: loss 0.1943, time 935.05ms, mfu 15.11%\n",
            "iter 3560: loss 0.1904, time 932.97ms, mfu 15.19%\n",
            "iter 3570: loss 0.2017, time 953.07ms, mfu 15.24%\n",
            "iter 3580: loss 0.2013, time 933.89ms, mfu 15.31%\n",
            "iter 3590: loss 0.1928, time 959.79ms, mfu 15.33%\n",
            "iter 3600: loss 0.1973, time 930.06ms, mfu 15.40%\n",
            "iter 3610: loss 0.1972, time 947.36ms, mfu 15.43%\n",
            "iter 3620: loss 0.1925, time 962.76ms, mfu 15.44%\n",
            "iter 3630: loss 0.1992, time 937.18ms, mfu 15.49%\n",
            "iter 3640: loss 0.1926, time 931.93ms, mfu 15.54%\n",
            "iter 3650: loss 0.1992, time 926.00ms, mfu 15.59%\n",
            "iter 3660: loss 0.1931, time 943.95ms, mfu 15.61%\n",
            "iter 3670: loss 0.1893, time 941.23ms, mfu 15.63%\n",
            "iter 3680: loss 0.1897, time 928.98ms, mfu 15.68%\n",
            "iter 3690: loss 0.1913, time 944.16ms, mfu 15.69%\n",
            "iter 3700: loss 0.1891, time 928.25ms, mfu 15.72%\n",
            "iter 3710: loss 0.1907, time 927.71ms, mfu 15.76%\n",
            "iter 3720: loss 0.1933, time 965.95ms, mfu 15.73%\n",
            "iter 3730: loss 0.1970, time 928.54ms, mfu 15.76%\n",
            "iter 3740: loss 0.1898, time 931.48ms, mfu 15.78%\n",
            "step 3750: train loss 0.0748, val loss 3.5169\n",
            "iter 3750: loss 0.1906, time 4049.58ms, mfu 14.57%\n",
            "iter 3760: loss 0.1831, time 930.67ms, mfu 14.72%\n",
            "iter 3770: loss 0.1950, time 935.91ms, mfu 14.84%\n",
            "iter 3780: loss 0.1908, time 926.16ms, mfu 14.96%\n",
            "iter 3790: loss 0.1908, time 931.47ms, mfu 15.07%\n",
            "iter 3800: loss 0.1917, time 948.68ms, mfu 15.13%\n",
            "iter 3810: loss 0.1831, time 963.06ms, mfu 15.17%\n",
            "iter 3820: loss 0.1773, time 927.96ms, mfu 15.26%\n",
            "iter 3830: loss 0.1874, time 958.56ms, mfu 15.28%\n",
            "iter 3840: loss 0.1856, time 935.43ms, mfu 15.35%\n",
            "iter 3850: loss 0.1844, time 934.36ms, mfu 15.41%\n",
            "iter 3860: loss 0.1807, time 938.36ms, mfu 15.46%\n",
            "iter 3870: loss 0.1888, time 937.56ms, mfu 15.50%\n",
            "iter 3880: loss 0.1968, time 937.91ms, mfu 15.54%\n",
            "iter 3890: loss 0.1773, time 926.68ms, mfu 15.59%\n",
            "iter 3900: loss 0.1789, time 952.67ms, mfu 15.60%\n",
            "iter 3910: loss 0.1878, time 937.93ms, mfu 15.63%\n",
            "iter 3920: loss 0.1848, time 938.29ms, mfu 15.65%\n",
            "iter 3930: loss 0.1889, time 947.41ms, mfu 15.66%\n",
            "iter 3940: loss 0.1889, time 923.52ms, mfu 15.71%\n",
            "iter 3950: loss 0.1857, time 946.39ms, mfu 15.71%\n",
            "iter 3960: loss 0.1853, time 947.62ms, mfu 15.72%\n",
            "iter 3970: loss 0.1802, time 948.33ms, mfu 15.72%\n",
            "iter 3980: loss 0.1774, time 922.21ms, mfu 15.76%\n",
            "iter 3990: loss 0.1803, time 942.27ms, mfu 15.77%\n",
            "step 4000: train loss 0.0738, val loss 3.5751\n",
            "iter 4000: loss 0.1848, time 4088.79ms, mfu 14.55%\n",
            "iter 4010: loss 0.1817, time 961.70ms, mfu 14.65%\n",
            "iter 4020: loss 0.1790, time 933.35ms, mfu 14.78%\n",
            "iter 4030: loss 0.1841, time 938.29ms, mfu 14.89%\n",
            "iter 4040: loss 0.1781, time 947.93ms, mfu 14.97%\n",
            "iter 4050: loss 0.1848, time 935.09ms, mfu 15.07%\n",
            "iter 4060: loss 0.1878, time 943.45ms, mfu 15.14%\n",
            "iter 4070: loss 0.1799, time 926.68ms, mfu 15.24%\n",
            "iter 4080: loss 0.1797, time 938.64ms, mfu 15.30%\n",
            "iter 4090: loss 0.1896, time 935.09ms, mfu 15.37%\n",
            "iter 4100: loss 0.1786, time 939.72ms, mfu 15.42%\n",
            "iter 4110: loss 0.1878, time 1012.52ms, mfu 15.35%\n",
            "iter 4120: loss 0.1790, time 917.69ms, mfu 15.44%\n",
            "iter 4130: loss 0.1820, time 927.22ms, mfu 15.50%\n",
            "iter 4140: loss 0.1842, time 937.58ms, mfu 15.54%\n",
            "iter 4150: loss 0.1779, time 946.63ms, mfu 15.56%\n",
            "iter 4160: loss 0.1820, time 930.89ms, mfu 15.60%\n",
            "iter 4170: loss 0.1775, time 941.86ms, mfu 15.63%\n",
            "iter 4180: loss 0.1809, time 939.32ms, mfu 15.65%\n",
            "iter 4190: loss 0.1745, time 928.43ms, mfu 15.69%\n",
            "iter 4200: loss 0.1801, time 921.85ms, mfu 15.74%\n",
            "iter 4210: loss 0.1739, time 953.88ms, mfu 15.73%\n",
            "iter 4220: loss 0.1836, time 934.61ms, mfu 15.75%\n",
            "iter 4230: loss 0.1767, time 928.81ms, mfu 15.78%\n",
            "iter 4240: loss 0.1843, time 942.66ms, mfu 15.78%\n",
            "step 4250: train loss 0.0730, val loss 3.6222\n",
            "iter 4250: loss 0.1800, time 4078.53ms, mfu 14.57%\n",
            "iter 4260: loss 0.1739, time 951.68ms, mfu 14.68%\n",
            "iter 4270: loss 0.1779, time 931.71ms, mfu 14.81%\n",
            "iter 4280: loss 0.1765, time 928.94ms, mfu 14.93%\n",
            "iter 4290: loss 0.1741, time 945.40ms, mfu 15.02%\n",
            "iter 4300: loss 0.1765, time 933.66ms, mfu 15.11%\n",
            "iter 4310: loss 0.1701, time 930.75ms, mfu 15.20%\n",
            "iter 4320: loss 0.1820, time 956.92ms, mfu 15.24%\n",
            "iter 4330: loss 0.1678, time 923.05ms, mfu 15.33%\n",
            "iter 4340: loss 0.1833, time 931.51ms, mfu 15.40%\n",
            "iter 4350: loss 0.1791, time 930.06ms, mfu 15.46%\n",
            "iter 4360: loss 0.1722, time 930.30ms, mfu 15.52%\n",
            "iter 4370: loss 0.1719, time 935.83ms, mfu 15.56%\n",
            "iter 4380: loss 0.1788, time 944.12ms, mfu 15.58%\n",
            "iter 4390: loss 0.1774, time 957.96ms, mfu 15.58%\n",
            "iter 4400: loss 0.1733, time 930.34ms, mfu 15.62%\n",
            "iter 4410: loss 0.1728, time 952.92ms, mfu 15.62%\n",
            "iter 4420: loss 0.1776, time 935.46ms, mfu 15.66%\n",
            "iter 4430: loss 0.1720, time 922.66ms, mfu 15.71%\n",
            "iter 4440: loss 0.1739, time 938.69ms, mfu 15.72%\n",
            "iter 4450: loss 0.1646, time 922.94ms, mfu 15.77%\n",
            "iter 4460: loss 0.1655, time 959.59ms, mfu 15.74%\n",
            "iter 4470: loss 0.1739, time 955.97ms, mfu 15.73%\n",
            "iter 4480: loss 0.1730, time 939.58ms, mfu 15.74%\n",
            "iter 4490: loss 0.1714, time 935.73ms, mfu 15.76%\n",
            "step 4500: train loss 0.0722, val loss 3.6456\n",
            "iter 4500: loss 0.1772, time 4065.31ms, mfu 14.55%\n",
            "iter 4510: loss 0.1686, time 935.11ms, mfu 14.69%\n",
            "iter 4520: loss 0.1752, time 946.70ms, mfu 14.79%\n",
            "iter 4530: loss 0.1673, time 943.40ms, mfu 14.89%\n",
            "iter 4540: loss 0.1768, time 927.01ms, mfu 15.01%\n",
            "iter 4550: loss 0.1653, time 931.17ms, mfu 15.11%\n",
            "iter 4560: loss 0.1695, time 926.63ms, mfu 15.21%\n",
            "iter 4570: loss 0.1683, time 937.74ms, mfu 15.28%\n",
            "iter 4580: loss 0.1670, time 926.19ms, mfu 15.36%\n",
            "iter 4590: loss 0.1700, time 933.60ms, mfu 15.42%\n",
            "iter 4600: loss 0.1721, time 944.49ms, mfu 15.46%\n",
            "iter 4610: loss 0.1703, time 932.70ms, mfu 15.51%\n",
            "iter 4620: loss 0.1721, time 940.97ms, mfu 15.54%\n",
            "iter 4630: loss 0.1761, time 938.62ms, mfu 15.58%\n",
            "iter 4640: loss 0.1720, time 939.35ms, mfu 15.60%\n",
            "iter 4650: loss 0.1724, time 933.17ms, mfu 15.64%\n",
            "iter 4660: loss 0.1719, time 945.73ms, mfu 15.65%\n",
            "iter 4670: loss 0.1754, time 949.11ms, mfu 15.66%\n",
            "iter 4680: loss 0.1677, time 931.71ms, mfu 15.69%\n",
            "iter 4690: loss 0.1631, time 933.66ms, mfu 15.72%\n",
            "iter 4700: loss 0.1652, time 954.63ms, mfu 15.71%\n",
            "iter 4710: loss 0.1709, time 933.22ms, mfu 15.74%\n",
            "iter 4720: loss 0.1781, time 934.77ms, mfu 15.76%\n",
            "iter 4730: loss 0.1728, time 936.08ms, mfu 15.77%\n",
            "iter 4740: loss 0.1683, time 928.47ms, mfu 15.80%\n",
            "step 4750: train loss 0.0718, val loss 3.6920\n",
            "iter 4750: loss 0.1706, time 4071.00ms, mfu 14.59%\n",
            "iter 4760: loss 0.1631, time 939.70ms, mfu 14.71%\n",
            "iter 4770: loss 0.1639, time 931.58ms, mfu 14.84%\n",
            "iter 4780: loss 0.1652, time 934.91ms, mfu 14.95%\n",
            "iter 4790: loss 0.1651, time 932.02ms, mfu 15.06%\n",
            "iter 4800: loss 0.1700, time 926.47ms, mfu 15.16%\n",
            "iter 4810: loss 0.1639, time 947.91ms, mfu 15.22%\n",
            "iter 4820: loss 0.1584, time 934.57ms, mfu 15.29%\n",
            "iter 4830: loss 0.1731, time 929.34ms, mfu 15.36%\n",
            "iter 4840: loss 0.1683, time 945.54ms, mfu 15.40%\n",
            "iter 4850: loss 0.1706, time 948.64ms, mfu 15.44%\n",
            "iter 4860: loss 0.1733, time 931.37ms, mfu 15.49%\n",
            "iter 4870: loss 0.1737, time 926.28ms, mfu 15.55%\n",
            "iter 4880: loss 0.1691, time 996.37ms, mfu 15.49%\n",
            "iter 4890: loss 0.1666, time 929.06ms, mfu 15.55%\n",
            "iter 4900: loss 0.1692, time 936.89ms, mfu 15.58%\n",
            "iter 4910: loss 0.1648, time 939.67ms, mfu 15.61%\n",
            "iter 4920: loss 0.1662, time 933.46ms, mfu 15.65%\n",
            "iter 4930: loss 0.1691, time 928.21ms, mfu 15.69%\n",
            "iter 4940: loss 0.1626, time 939.22ms, mfu 15.71%\n",
            "iter 4950: loss 0.1632, time 940.06ms, mfu 15.72%\n",
            "iter 4960: loss 0.1689, time 936.08ms, mfu 15.74%\n",
            "iter 4970: loss 0.1772, time 933.56ms, mfu 15.76%\n",
            "iter 4980: loss 0.1643, time 968.77ms, mfu 15.73%\n",
            "iter 4990: loss 0.1642, time 933.70ms, mfu 15.75%\n",
            "step 5000: train loss 0.0715, val loss 3.7191\n",
            "iter 5000: loss 0.1677, time 4062.09ms, mfu 14.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sample.py : \n",
        "train 후 sample.py로 GPT 테스트 \n",
        "\n",
        "1. 하이퍼 파라메터\n",
        "\n",
        "   num_samples = 10 # number of samples to draw / \n",
        "max_new_tokens = 500 # number of tokens generated in each sample  /\n",
        "temperature = 0.8 # 는 생성된 단어의 다양성을 제어하는 역할 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions / \n",
        "top_k = 200 # 후보 단어 집합에서 선택할 단어의 개수를 제한하는 역할 retain only the top_k most likely tokens, clamp others to have 0 probability / \n",
        "seed = 1337\n",
        "\n",
        "2. model : GPT 설정 후 모델의 파라미터들을 불러옴.\n",
        "    model = GPT(gptconf)\n",
        "\n",
        "    state_dict = checkpoint['model']\n",
        "  \n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "3. model.eval() : 모델의 파라미터들이 고정된 상태로 설정되고, dropout과 batch normalization과 같은 연산들도 eval 상태로 변경\n",
        "\n",
        " 메타 파일은 data/shakespeare_char/meta.pkl을 사용\n",
        "\n",
        "4. 자연어 생성 :  \n",
        "  num_samples 10개 만큼 , max_new_tokens = 500, temperature = 0.8, top_k = 200\n",
        "   \n",
        "   for k in range(num_samples): y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "\n"
      ],
      "metadata": {
        "id": "gFDiC98CWFfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run sample.py --out_dir=out-shakespeare-char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR8IxUGs6ZOK",
        "outputId": "d6938724-e91e-44d4-9369-01a2c31b51b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "Say they stay a service, and all the streaginates:\n",
            "Whither 'twas he that did him enjoy?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "What with the way come matter?\n",
            "\n",
            "KING RICHARD III:\n",
            "Old George overthanks, methinks, that we learn you with us.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "My woyfully sister for this death:\n",
            "I will make keep Warwick, their king arm down.\n",
            "\n",
            "YORK:\n",
            "I can poor great to hear him, they find;\n",
            "And he must not know him from Prince, Somerset,\n",
            "Let's too dischard a curse of the discontrary\n",
            "And his hand, and for hi\n",
            "---------------\n",
            "\n",
            "Menenius, sir, you shall have hand her;\n",
            "And the noble two men reason; and being with weeping,\n",
            "The score something casely to England,\n",
            "My worst shall be beloved to die.\n",
            "\n",
            "PRINCE:\n",
            "And I cannot see the king in my lands:\n",
            "I come, sir, sir, then it die now, for the child.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Well, then long a despised by my life;\n",
            "For he shall desire be the napking gently\n",
            "That hath deius disordered eyes me with the city;\n",
            "And so the silver were believe so harm,\n",
            "And bear the leisure of the truth\n",
            "And which \n",
            "---------------\n",
            "\n",
            "MARIANA:\n",
            "I beseech you, must no more.\n",
            "My lord, how can I was by your back\n",
            "A sharp-please with mine own leave yield, in the heart,\n",
            "How would we call his good hand wIn all a cause\n",
            "Of a long come above to do withal\n",
            "Burning Reinealine and my hand; and I did the detecty\n",
            "By His head of Bluntague. The old conscience hath not crown'd?\n",
            "\n",
            "Second Keeper:\n",
            "Be it so dispatch'd upon him, where he could\n",
            "The war in this field of mine honest have waited\n",
            "The worstman's eyes of hell, get itself you,\n",
            "But when he she \n",
            "---------------\n",
            "\n",
            "\n",
            "First Servingman:\n",
            "A thousand a fair;\n",
            "for whether that can rest!\n",
            "\n",
            "Second Servingman:\n",
            "But she have spoke deserved thee thy party hand.\n",
            "\n",
            "First Servingman:\n",
            "How doth a heart give a head of me?\n",
            "\n",
            "CORIOLANUS:\n",
            "A\n",
            "CORIOLANUS:\n",
            "Now her could have colour'd the war\n",
            "That I could take thee him.\n",
            "\n",
            "Second Servingman:\n",
            "We have bare a sorrow's sister's own\n",
            "Alike he had crown'd it and wakingly:\n",
            "And say 'By they promise the minning\n",
            "I was hath been so of the harmation.\n",
            "\n",
            "First Senator:\n",
            "I have none more for her beholdings\n",
            "---------------\n",
            "\n",
            "Be ever but were they lost to be continued\n",
            "Of my death, an instruction, but the enemy?\n",
            "\n",
            "BRUTUS:\n",
            "No; which, what we may be conceived\n",
            "To the deep consul\n",
            "The country is so fair death.\n",
            "\n",
            "CORIOLANUS:\n",
            "Look,\n",
            "And you were not with us.\n",
            "\n",
            "First Senator:\n",
            "I will hence you can with the man of strong,\n",
            "And some ancient your courts won majesty;\n",
            "Being as you are come as known'd as my part,\n",
            "And to help me be residered.\n",
            "\n",
            "Second Servingman:\n",
            "But, this is the air down to your friendship sleep him;\n",
            "And there forget you \n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "He hath he stain'd it, he must have true meltier.\n",
            "\n",
            "BRUTUS:\n",
            "Care now, conceive me and set the general shall\n",
            "The portion.\n",
            "\n",
            "SICINIUS:\n",
            "You cannot deliver stop.\n",
            "\n",
            "VIRGILIA:\n",
            "Ay, will I ingeneral prison.\n",
            "\n",
            "CORIOLANUS:\n",
            "Believe me, the crown kingdom the cousin,\n",
            "That bitter makes, here between another and woe.\n",
            "\n",
            "BRUTUS:\n",
            "Certain you come hither, I have dispose\n",
            "The promise of you.\n",
            "\n",
            "SICINIUS:\n",
            "You have like to you;\n",
            "Yet by you have made it not.\n",
            "\n",
            "SICINIUS:\n",
            "Sir, if you had rather worse your\n",
            "father, and t\n",
            "---------------\n",
            "\n",
            "Shepherd:\n",
            "Here her blood against the set,\n",
            "And so says her story to her good.\n",
            "\n",
            "POLIXENES:\n",
            "Let's hands have been found:\n",
            "A stored prove son, word cracking true times\n",
            "And make it tready more or hence.\n",
            "\n",
            "PERDITA:\n",
            "Then I love the Volsces nor of Margaret's general,\n",
            "And that can be married with the worstic lie.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "O very walls our own reasons, and true shall be a wine\n",
            "to meet his talk of his dear child, and make him known\n",
            "To have a spirit of the city gates.\n",
            "\n",
            "ROMEO:\n",
            "Alack, what, little souls\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "Therefore that he is not not.\n",
            "\n",
            "ANGELO:\n",
            "You are abused, good Lord:\n",
            "These common part the earth of my love my necess sound,\n",
            "And, behave my father love known toders;\n",
            "And yet I have desire heard my sword\n",
            "My heart of my appetite unto King Henry's side,\n",
            "That heart ever been so death there; but now, the posterity\n",
            "Take him come the head of woman, were thy hand,\n",
            "And thou didst keep him for thy husband.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Exeter, the liege is thus with him wrong;\n",
            "And, and men had with Lady Henry C\n",
            "---------------\n",
            "\n",
            "\n",
            "LEONTES:\n",
            "The Lord Antigonus Titus,\n",
            "To bear the molehildings Romeo cruel vanity\n",
            "Hath been in their state brains, and shall be believed,\n",
            "And in the tree that hath been, he will have powered their\n",
            "That hath nothing cheers, is letter there.\n",
            "There is no more crack with the body of our house,\n",
            "When he had said on the king's, whose breasts upon their breaths,\n",
            "The gravest in their grace or terror their own regard\n",
            "The day of this duke pities in the hands.\n",
            "Look, thus have deserved to tell me, if they were\n",
            "---------------\n",
            "\n",
            "QUEEN MARGARET:\n",
            "From what he shall not for this face.\n",
            "What should be truly shame! you slave, it is this name?\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Why, brother, I know bethink you?\n",
            "\n",
            "PRINCE EDWARD:\n",
            "\n",
            "GLOUCESTER:\n",
            "What! how what says were heaven and friends?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Go to, give you next the cold,\n",
            "Receive not stars that rumour to your will?\n",
            "\n",
            "GLOUCESTER:\n",
            "So do his foe too?\n",
            "\n",
            "YORK:\n",
            "Ay, my lord, when we may you this lady?\n",
            "\n",
            "KING EDWARD IV:\n",
            "But then, that you have not spoke to the king.\n",
            "\n",
            "YORK:\n",
            "That be heavy both the \n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**finetuning**\n",
        "미세조정은 train과 다르지 않고 pre-trained 된 모델을 초기화하고 더 작은 학습 속도로 train 한다.\n",
        "\n",
        "## train.py\n",
        "1. confing file : finetune_shakespeare.py\n",
        "  \n",
        "  1.1 eval_interval = 5 / \n",
        "eval_iters = 40 /\n",
        "dataset = 'shakespeare'\n",
        "init_from = 'gpt2-xl' # this is the largest GPT-2 model\n",
        "\n",
        "  1.2 total number of tokens per iteration (batch_size * block_size * radient_accumulation_steps ) : 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter --> 실제 아래 train.py 수행 시는 config 파일 주석과 달리 262,144 개가 수행 됨\n",
        "\n",
        "  shakespeare 301,966 tokens 있어서 1 epoch ~= 9.2 iteratjons 수행행\n",
        "\n",
        "  batch_size = 1 / \n",
        "gradient_accumulation_steps = 32 / \n",
        "max_iters = 20\n",
        "\n",
        "  1.3 finetune at constant LR : \n",
        "learning_rate = 3e-5  / \n",
        "decay_lr = False\n",
        "\n",
        "2. train.py : config/finetune_shakespeare.py config 파일을 이용해 gpt2-xl train\n",
        "\n",
        " 2.1 위의 'quick start'에서 생성된 train.bin과 val.bin를 복사 해서 다운로드 해 둔다.\n",
        "\n",
        " 2.2 위의 'quick start'의 train과 같은 프로그램인 train.py를 finetune_shakespeare.py로 finetuning하여 수행\n",
        "\n",
        " total number of tokens per iteration: 262144\n",
        "\n",
        " OpenAI GPT-2 weights: gpt2-xl\n",
        "\n",
        " forcing vocab_size=50257, block_size=1024, bias=True\n",
        "\n",
        " number of parameters: 1555.97M\n",
        "\n",
        " using fused AdamW: True \n",
        " - AdamW는 Adam(Adaptive Moment Estimation) 옵티마이저의 변형으로, 가중치 감쇠(weight decay)를 수행하는 방법이며 Adam의 가중치 감쇠와 다르게, 가중치 감쇠를 weight decay만으로 수행하지 않고, 가중치 감쇠와 optimizer의 파라미터 감쇠를 결합하여 수행합니다. 이를 통해 모델 파라미터의 편향(bias)에 대한 가중치 감쇠를 수행하는 동시에, 가중치 감쇠를 수행하는 것과 optimizer의 파라미터 감쇠를 따로 적용하는 것보다 더욱 좋은 성능을 보임\n",
        "\n",
        "2.3 iteration 20번 실행 후 최적의  train loss / val loss : step 20 : train loss 1.9367, val loss 1.9109\n"
      ],
      "metadata": {
        "id": "ZGGXnjalqSNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'quick start'에서 생성된 train.bin과 val.bin를 복사 해서 다운로드 해 둔다."
      ],
      "metadata": {
        "id": "3qW_JXGdKk_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy('/content/nanoGPT/data/shakespeare_char/train.bin', '/content/nanoGPT/data/shakespeare/')\n",
        "shutil.copy('/content/nanoGPT/data/shakespeare_char/val.bin', '/content/nanoGPT/data/shakespeare/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xIMGIParJ2bl",
        "outputId": "35e7e74f-8da3-44f8-c21f-29c0c86873e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/nanoGPT/data/shakespeare/val.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/nanoGPT/data/shakespeare/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNy9_DARKe9I",
        "outputId": "c8ca3b81-3cdf-4009-e1a1-f14d925a08bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prepare.py  readme.md  train.bin  val.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run train.py config/finetune_shakespeare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aaee3e6a07264f0c87493efacb5cce54",
            "c924330425c14cfda6e9964b8c16a2e7",
            "71828f10bc4c4b0cbc826721905012f0",
            "bfd7a54d95134f788be6ed34cc68fae1",
            "1ed61472150d4689901e8e39ebdeaf36",
            "7e4a024003224ed2a8dd75684d8a2e43",
            "2eb3759984494068935fd1446391dd21",
            "f66454662f184fff8607959086791a57",
            "f98f3d6de78e41f58fd3a3573fc855cc",
            "b827a101863f43d2bb16f0cf5655fd8b",
            "4b1977f1b4b742f2a8471e62ec932709",
            "8b1dd2c764044dcea2f4c70cb1437014",
            "1c5a1c0779e14127ad9330415f61a1ab",
            "ef7c56afabbc484dbdb303f2cbdb2462",
            "b8150d47dc3c44e3ac0cc690c6e30119",
            "9c938c8dfcf74b5dbaf10904b7fa5a55",
            "e2d85f9ce6434d8a849d1a1a72ed9797",
            "0fd9893ad16542ce8dcd7c2d3e3bf760",
            "5e154a16770a4e91bc70292d0b4b4bb0",
            "79a0fa75118048d3b6475beb297a1b08",
            "b155db82d1f44befa5d214a24d67f960",
            "6ed8b71213924efd861d2ab1d5942fad",
            "638a7ada7ecd4108b5bf58ba958baef2",
            "e52ce19d3e5d44a5894e0c5f602e24a9",
            "2ffe9a1c7ec94776a351edec3dfa05f2",
            "b73dfdc6c7f8405f9dbadae090441465",
            "47af6a6377524fc5b99ce65257690afa",
            "6c48f07d2633442ca7a50eb7dbefcacd",
            "ac0797f03e6143e190a3e9765103c248",
            "69862f58211b4b09b3f0b0cf00b3a01b",
            "60ff20c9a6c54bcc87a90a0b4ba8998b",
            "e35cd8f6e0ff487d89f3363eb208d8b4",
            "3c8b63aaeef64f709bc57583c8a2472b"
          ]
        },
        "id": "iwqJ4OChjmr3",
        "outputId": "82838d81-a42f-40c1-a020-9725bc4653ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding config with config/finetune_shakespeare.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-shakespeare'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2-xl' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "total number of tokens per iteration: 262144\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-xl\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaee3e6a07264f0c87493efacb5cce54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b1dd2c764044dcea2f4c70cb1437014",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "638a7ada7ecd4108b5bf58ba958baef2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 3.8766, val loss 3.7893\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-04-24 02:52:12,330] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
            "   function: 'forward' (/content/nanoGPT/model.py:110)\n",
            "   reasons:  ___check_obj_id(self, 140596194586000)\n",
            "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
            "[2023-04-24 02:52:41,014] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
            "   function: 'forward' (/content/nanoGPT/model.py:34)\n",
            "   reasons:  ___check_obj_id(self, 140596194996960)\n",
            "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 3.8493, time 130696.06ms, mfu -100.00%\n",
            "iter 1: loss 3.2312, time 37948.27ms, mfu -100.00%\n",
            "iter 2: loss 3.1915, time 37796.50ms, mfu -100.00%\n",
            "iter 3: loss 2.8811, time 37874.83ms, mfu -100.00%\n",
            "iter 4: loss 2.5588, time 38048.24ms, mfu -100.00%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-04-24 02:56:23,267] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
            "   function: 'forward' (/content/nanoGPT/model.py:60)\n",
            "   reasons:  ___check_obj_id(self, 140596194586288)\n",
            "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
            "[2023-04-24 02:56:24,523] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
            "   function: 'forward' (/content/nanoGPT/model.py:94)\n",
            "   reasons:  ___check_obj_id(self, 140594753651376)\n",
            "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 5: train loss 2.5800, val loss 2.5622\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 5: loss 2.5740, time 102855.24ms, mfu 8.40%\n",
            "iter 6: loss 2.5469, time 39792.13ms, mfu 9.73%\n",
            "iter 7: loss 2.3779, time 39777.31ms, mfu 10.93%\n",
            "iter 8: loss 2.2152, time 39777.26ms, mfu 12.01%\n",
            "iter 9: loss 2.3448, time 39783.47ms, mfu 12.98%\n",
            "step 10: train loss 2.2956, val loss 2.2889\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 10: loss 2.3615, time 101504.21ms, mfu 12.53%\n",
            "iter 11: loss 2.2208, time 39789.24ms, mfu 13.45%\n",
            "iter 12: loss 2.2204, time 39771.99ms, mfu 14.27%\n",
            "iter 13: loss 2.2598, time 39769.26ms, mfu 15.02%\n",
            "iter 14: loss 2.1561, time 39782.97ms, mfu 15.69%\n",
            "step 15: train loss 2.1212, val loss 2.0522\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 15: loss 2.1180, time 101603.48ms, mfu 14.97%\n",
            "iter 16: loss 2.1617, time 39789.15ms, mfu 15.64%\n",
            "iter 17: loss 2.0797, time 39782.52ms, mfu 16.25%\n",
            "iter 18: loss 2.1993, time 39777.65ms, mfu 16.80%\n",
            "iter 19: loss 2.0556, time 39798.95ms, mfu 17.29%\n",
            "step 20: train loss 1.9367, val loss 1.9109\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 20: loss 1.9766, time 101171.67ms, mfu 16.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##finetuning Sample : \n",
        "finetuning 모델을 가지고 \"What is the answer to life, the universe, and everything?\" 질의에 대한 생성\n",
        "\n",
        "- 모델 설정 : init_from = gpt2-xl (the largest GPT-2 model) \n",
        "- 질의문장 : start = What is the answer to life, the universe, and everything? \n",
        "- 생성한 sample 수 : num_samples = 5\n",
        "- 각 샘플의 최대 생성 단어 : max_new_tokens = 100"
      ],
      "metadata": {
        "id": "FhTS3q3lfWfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=5 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb26NRpvDqdx",
        "outputId": "b1c5ce59-6ecf-4b33-db8e-03cab54fd3e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 100\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows for no contradictions. And, if so, then why is there contradiction?\n",
            "\n",
            "Another possibility is that the universe is one big joke. They are not the only joke in the universe. But it is not an empty universe.\n",
            "\n",
            "Hence the problems with the universe's Big Bang theory, which tells us that the universe started out in a singularity with a zero initial mass. The Big Bang is essentially consistent with\n",
            "---------------\n",
            "What is the answer to life, the universe, and everything? Probably not. Quantum physics is a mystery. It's not settled. So if more and more people start to believe it, it's going to become very hard to argue against. It seems to me that's the only way to learn more.\n",
            "\n",
            "There are a lot of people out there who think the way to get smarter is to get rid of our human brains, that they're too difficult to understand. But if we could get rid of our brains, that wouldn't make us smarter.\n",
            "---------------\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "\n",
            "Can a dog be made to love us?\n",
            "\n",
            "\n",
            "Are there aliens out there?\n",
            "\n",
            "\n",
            "If the bible is the word of god, what is the problem?\n",
            "\n",
            "\n",
            "If Christ was the Son of God, why didn't he save all those who died in his name?\n",
            "\n",
            "\n",
            "Does the bible really talk about hell?<|endoftext|>According to a representative from Sony Picture Entertainment, Sony Pictures Home Entertainment has no plans to release the Fullmetal Alchemist: Brotherhood on Blu-ray. The reason\n",
            "---------------\n",
            "What is the answer to life, the universe, and everything? In The Origin of Life, David Deutsch and Max Tegmark seek the answer by tackling some of the most difficult questions in science. Based on the first full-length biography of Michael Faraday, this original and entertaining book deals with not only the origin of the universe and life, but also the question of why we are here. Deutsch and Tegmark tell how the idea that life was seeded on earth was first proposed by scientists in the nineteenth century as part of the general approach to the origin\n",
            "---------------\n",
            "What is the answer to life, the universe, and everything? This question has vexed philosophers in all times and cultures.\n",
            "\n",
            "From ancient Greece to 16th-century Italy, philosophers and theologians sought the answer to this question. Some came to a conclusion; most came to a different one.\n",
            "\n",
            "Many of the best known of these thinkers were Bacon, Descartes, Newton, Leibniz, Leibniz's collaborator Gottfried Leibniz, and English philosopher Richard Swinburne. (The latter two, like\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**nanoGPT 활용해서 songWriter 작성해 보기**\n",
        "\n",
        "# dataset : \n",
        " - **Kaggle's Spotify Million Song Dataset** 사용 : spotify_millsongdata.csv\n",
        " - artist name, song name, link : song and lyrics, text\n",
        "\n",
        "## prepare.py : \n",
        "\n",
        "/data/shakespeare/prepare.py 변형\n",
        "\n",
        "songwriter를 위한 dataset file load : spotify_millsongdata.csv\n",
        " - 해당 엑셀의 'text' 컬럼으로 data 추출하여 train, val data 사용"
      ],
      "metadata": {
        "id": "dKyi_hgrtIp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/nanoGPT/data/songwriter/prepare.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSpMRZanvLAo",
        "outputId": "67b145ba-50ee-4fb4-e5f0-ee034e7082ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 22,308,928 tokens\n",
            "val has 2,456,916 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 prepare.py 실행 후 train.bin, val.bin 파일이 생성됨을 확인 할 수 있다."
      ],
      "metadata": {
        "id": "CmVC52RSzscm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/nanoGPT/data/songwriter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB43GnHqzmv6",
        "outputId": "428dcf98-972d-4031-c54a-2eb9ce0d4ca7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prepare.py  spotify_millsongdata.csv  train.bin  val.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py : \n",
        "config/train_shakespeare_char.py config 파일을 이용해 GPT train\n",
        "\n",
        "iteration 5000 번 수행 후 최적의 train loss/val loss : step 5000 train loss 1.6726, val loss 2.3793"
      ],
      "metadata": {
        "id": "ppE9emoBz9eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run train.py config/train_songwriter.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJpJbDjn0DAh",
        "outputId": "080d3e3a-53b0-416e-fff7-272db6bf2557"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_songwriter.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-songwriter'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'songwriter'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'songwriter'\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "\n",
            "total number of tokens per iteration: 655360\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 29.94M\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 10.7093, val loss 10.7104\n",
            "iter 0: loss 10.7263, time 11247.48ms, mfu -100.00%\n",
            "iter 10: loss 8.8672, time 1961.91ms, mfu 19.99%\n",
            "iter 20: loss 7.7545, time 1961.94ms, mfu 19.99%\n",
            "iter 30: loss 6.4958, time 1959.92ms, mfu 19.99%\n",
            "iter 40: loss 5.2347, time 1961.98ms, mfu 19.99%\n",
            "iter 50: loss 4.6231, time 1964.89ms, mfu 19.99%\n",
            "iter 60: loss 4.4471, time 1966.00ms, mfu 19.98%\n",
            "iter 70: loss 4.0679, time 1967.67ms, mfu 19.98%\n",
            "iter 80: loss 3.9799, time 1960.77ms, mfu 19.98%\n",
            "iter 90: loss 3.8164, time 1965.49ms, mfu 19.98%\n",
            "iter 100: loss 3.6908, time 1963.48ms, mfu 19.98%\n",
            "iter 110: loss 3.6524, time 1963.63ms, mfu 19.98%\n",
            "iter 120: loss 3.6535, time 1970.49ms, mfu 19.97%\n",
            "iter 130: loss 3.5773, time 1965.89ms, mfu 19.97%\n",
            "iter 140: loss 3.5660, time 1967.69ms, mfu 19.96%\n",
            "iter 150: loss 3.4576, time 1967.69ms, mfu 19.96%\n",
            "iter 160: loss 3.4908, time 1972.23ms, mfu 19.95%\n",
            "iter 170: loss 3.4854, time 1966.78ms, mfu 19.95%\n",
            "iter 180: loss 3.3144, time 1967.78ms, mfu 19.95%\n",
            "iter 190: loss 3.4534, time 1973.34ms, mfu 19.94%\n",
            "iter 200: loss 3.2798, time 1972.40ms, mfu 19.94%\n",
            "iter 210: loss 3.3199, time 1964.24ms, mfu 19.94%\n",
            "iter 220: loss 3.3790, time 1971.11ms, mfu 19.94%\n",
            "iter 230: loss 3.3300, time 1969.06ms, mfu 19.93%\n",
            "iter 240: loss 3.1833, time 1975.26ms, mfu 19.93%\n",
            "step 250: train loss 3.0620, val loss 3.2113\n",
            "saving checkpoint to out-songwriter\n",
            "iter 250: loss 3.1656, time 11457.36ms, mfu 18.28%\n",
            "iter 260: loss 3.1751, time 1966.44ms, mfu 18.44%\n",
            "iter 270: loss 2.9969, time 1971.88ms, mfu 18.59%\n",
            "iter 280: loss 2.9938, time 1971.27ms, mfu 18.72%\n",
            "iter 290: loss 3.1809, time 1964.41ms, mfu 18.84%\n",
            "iter 300: loss 2.8910, time 1966.02ms, mfu 18.95%\n",
            "iter 310: loss 2.9557, time 1967.58ms, mfu 19.05%\n",
            "iter 320: loss 2.9527, time 1963.57ms, mfu 19.14%\n",
            "iter 330: loss 2.9734, time 1963.80ms, mfu 19.23%\n",
            "iter 340: loss 2.8583, time 1968.93ms, mfu 19.30%\n",
            "iter 350: loss 2.9038, time 1965.29ms, mfu 19.36%\n",
            "iter 360: loss 2.8333, time 1960.74ms, mfu 19.43%\n",
            "iter 370: loss 2.7477, time 1966.42ms, mfu 19.48%\n",
            "iter 380: loss 2.9674, time 1964.37ms, mfu 19.53%\n",
            "iter 390: loss 2.8234, time 1966.84ms, mfu 19.57%\n",
            "iter 400: loss 2.7705, time 1962.52ms, mfu 19.61%\n",
            "iter 410: loss 2.8245, time 1965.44ms, mfu 19.64%\n",
            "iter 420: loss 2.8704, time 1966.65ms, mfu 19.67%\n",
            "iter 430: loss 2.5955, time 1965.96ms, mfu 19.70%\n",
            "iter 440: loss 2.6550, time 1967.83ms, mfu 19.72%\n",
            "iter 450: loss 2.7058, time 1967.74ms, mfu 19.74%\n",
            "iter 460: loss 2.7077, time 1976.80ms, mfu 19.75%\n",
            "iter 470: loss 2.7283, time 1963.26ms, mfu 19.78%\n",
            "iter 480: loss 2.5767, time 1968.91ms, mfu 19.79%\n",
            "iter 490: loss 2.5455, time 1961.77ms, mfu 19.81%\n",
            "step 500: train loss 2.4261, val loss 2.6385\n",
            "saving checkpoint to out-songwriter\n",
            "iter 500: loss 2.4869, time 11771.66ms, mfu 18.16%\n",
            "iter 510: loss 2.5857, time 1964.20ms, mfu 18.34%\n",
            "iter 520: loss 2.5864, time 1965.13ms, mfu 18.50%\n",
            "iter 530: loss 2.4890, time 1969.53ms, mfu 18.65%\n",
            "iter 540: loss 2.4760, time 1961.35ms, mfu 18.78%\n",
            "iter 550: loss 2.3370, time 1972.03ms, mfu 18.89%\n",
            "iter 560: loss 2.4855, time 1960.87ms, mfu 19.00%\n",
            "iter 570: loss 2.4803, time 1968.01ms, mfu 19.09%\n",
            "iter 580: loss 2.4987, time 1968.39ms, mfu 19.18%\n",
            "iter 590: loss 2.4083, time 1965.61ms, mfu 19.26%\n",
            "iter 600: loss 2.4417, time 1964.26ms, mfu 19.33%\n",
            "iter 610: loss 2.4038, time 1967.77ms, mfu 19.39%\n",
            "iter 620: loss 2.4302, time 1964.27ms, mfu 19.44%\n",
            "iter 630: loss 2.4274, time 1961.65ms, mfu 19.50%\n",
            "iter 640: loss 2.4260, time 1960.65ms, mfu 19.55%\n",
            "iter 650: loss 2.3836, time 1963.54ms, mfu 19.59%\n",
            "iter 660: loss 2.4202, time 1970.66ms, mfu 19.62%\n",
            "iter 670: loss 2.4255, time 1965.61ms, mfu 19.66%\n",
            "iter 680: loss 2.3292, time 1966.45ms, mfu 19.68%\n",
            "iter 690: loss 2.4718, time 1964.85ms, mfu 19.71%\n",
            "iter 700: loss 2.3180, time 1968.24ms, mfu 19.73%\n",
            "iter 710: loss 2.2454, time 1962.47ms, mfu 19.76%\n",
            "iter 720: loss 2.2512, time 1971.51ms, mfu 19.77%\n",
            "iter 730: loss 2.2399, time 1963.32ms, mfu 19.79%\n",
            "iter 740: loss 2.4032, time 1978.94ms, mfu 19.79%\n",
            "step 750: train loss 2.2142, val loss 2.4554\n",
            "saving checkpoint to out-songwriter\n",
            "iter 750: loss 2.2897, time 11735.27ms, mfu 18.15%\n",
            "iter 760: loss 2.2857, time 1966.23ms, mfu 18.33%\n",
            "iter 770: loss 2.3323, time 1973.00ms, mfu 18.48%\n",
            "iter 780: loss 2.2668, time 1964.82ms, mfu 18.63%\n",
            "iter 790: loss 2.4270, time 1968.74ms, mfu 18.76%\n",
            "iter 800: loss 2.2891, time 1964.40ms, mfu 18.88%\n",
            "iter 810: loss 2.2393, time 1967.00ms, mfu 18.99%\n",
            "iter 820: loss 2.3382, time 1964.23ms, mfu 19.08%\n",
            "iter 830: loss 2.2471, time 1977.67ms, mfu 19.16%\n",
            "iter 840: loss 2.2909, time 1964.84ms, mfu 19.24%\n",
            "iter 850: loss 2.4892, time 1966.12ms, mfu 19.31%\n",
            "iter 860: loss 2.1755, time 1966.25ms, mfu 19.37%\n",
            "iter 870: loss 2.2460, time 1965.63ms, mfu 19.43%\n",
            "iter 880: loss 2.3623, time 1959.02ms, mfu 19.49%\n",
            "iter 890: loss 2.2007, time 1962.03ms, mfu 19.54%\n",
            "iter 900: loss 2.1749, time 1963.73ms, mfu 19.58%\n",
            "iter 910: loss 2.2459, time 1966.74ms, mfu 19.62%\n",
            "iter 920: loss 2.1201, time 1967.65ms, mfu 19.65%\n",
            "iter 930: loss 2.1230, time 1960.64ms, mfu 19.69%\n",
            "iter 940: loss 2.1791, time 1963.30ms, mfu 19.71%\n",
            "iter 950: loss 2.2419, time 1963.37ms, mfu 19.74%\n",
            "iter 960: loss 2.2007, time 1969.87ms, mfu 19.76%\n",
            "iter 970: loss 2.0746, time 1962.95ms, mfu 19.78%\n",
            "iter 980: loss 2.2011, time 1968.56ms, mfu 19.79%\n",
            "iter 990: loss 2.2294, time 1968.50ms, mfu 19.81%\n",
            "step 1000: train loss 2.0804, val loss 2.3850\n",
            "saving checkpoint to out-songwriter\n",
            "iter 1000: loss 2.0870, time 11757.96ms, mfu 18.16%\n",
            "iter 1010: loss 2.2669, time 1966.19ms, mfu 18.34%\n",
            "iter 1020: loss 2.2686, time 1966.14ms, mfu 18.50%\n",
            "iter 1030: loss 2.2475, time 1962.74ms, mfu 18.65%\n",
            "iter 1040: loss 2.3043, time 1961.69ms, mfu 18.78%\n",
            "iter 1050: loss 2.1304, time 1968.99ms, mfu 18.90%\n",
            "iter 1060: loss 2.0933, time 1963.46ms, mfu 19.00%\n",
            "iter 1070: loss 2.2393, time 1967.64ms, mfu 19.10%\n",
            "iter 1080: loss 2.1298, time 1964.63ms, mfu 19.18%\n",
            "iter 1090: loss 2.0860, time 1967.72ms, mfu 19.26%\n",
            "iter 1100: loss 2.1876, time 1967.50ms, mfu 19.33%\n",
            "iter 1110: loss 2.1194, time 1966.28ms, mfu 19.39%\n",
            "iter 1120: loss 2.1440, time 1964.52ms, mfu 19.44%\n",
            "iter 1130: loss 2.1160, time 1969.12ms, mfu 19.49%\n",
            "iter 1140: loss 2.1826, time 1966.24ms, mfu 19.54%\n",
            "iter 1150: loss 2.2392, time 1960.50ms, mfu 19.58%\n",
            "iter 1160: loss 2.1350, time 1962.10ms, mfu 19.62%\n",
            "iter 1170: loss 2.2084, time 1969.29ms, mfu 19.65%\n",
            "iter 1180: loss 2.1628, time 1969.60ms, mfu 19.68%\n",
            "iter 1190: loss 2.0636, time 1963.30ms, mfu 19.71%\n",
            "iter 1200: loss 2.1679, time 1962.20ms, mfu 19.74%\n",
            "iter 1210: loss 2.1099, time 1960.02ms, mfu 19.76%\n",
            "iter 1220: loss 2.0697, time 1968.93ms, mfu 19.78%\n",
            "iter 1230: loss 2.0623, time 1960.41ms, mfu 19.80%\n",
            "iter 1240: loss 2.0802, time 1970.74ms, mfu 19.81%\n",
            "step 1250: train loss 1.9918, val loss 2.3516\n",
            "saving checkpoint to out-songwriter\n",
            "iter 1250: loss 2.1731, time 11743.54ms, mfu 18.16%\n",
            "iter 1260: loss 2.0598, time 1962.77ms, mfu 18.35%\n",
            "iter 1270: loss 2.0782, time 1965.35ms, mfu 18.51%\n",
            "iter 1280: loss 2.2101, time 1962.29ms, mfu 18.66%\n",
            "iter 1290: loss 2.1003, time 1965.52ms, mfu 18.78%\n",
            "iter 1300: loss 2.0632, time 1965.78ms, mfu 18.90%\n",
            "iter 1310: loss 2.1253, time 1966.16ms, mfu 19.01%\n",
            "iter 1320: loss 2.0426, time 1962.10ms, mfu 19.10%\n",
            "iter 1330: loss 2.1616, time 1966.14ms, mfu 19.19%\n",
            "iter 1340: loss 2.1485, time 1964.90ms, mfu 19.27%\n",
            "iter 1350: loss 2.1241, time 1966.85ms, mfu 19.33%\n",
            "iter 1360: loss 2.1010, time 1962.18ms, mfu 19.40%\n",
            "iter 1370: loss 1.9216, time 1969.86ms, mfu 19.45%\n",
            "iter 1380: loss 2.1366, time 1965.77ms, mfu 19.50%\n",
            "iter 1390: loss 2.1340, time 1963.36ms, mfu 19.55%\n",
            "iter 1400: loss 2.1120, time 1963.96ms, mfu 19.59%\n",
            "iter 1410: loss 2.0401, time 1959.86ms, mfu 19.63%\n",
            "iter 1420: loss 2.0724, time 1962.31ms, mfu 19.67%\n",
            "iter 1430: loss 2.0435, time 1963.98ms, mfu 19.70%\n",
            "iter 1440: loss 2.0937, time 1955.83ms, mfu 19.73%\n",
            "iter 1450: loss 2.1025, time 1965.58ms, mfu 19.75%\n",
            "iter 1460: loss 2.0544, time 1967.57ms, mfu 19.77%\n",
            "iter 1470: loss 2.0566, time 1964.97ms, mfu 19.79%\n",
            "iter 1480: loss 2.0149, time 1969.72ms, mfu 19.80%\n",
            "iter 1490: loss 2.0170, time 1965.36ms, mfu 19.82%\n",
            "step 1500: train loss 1.9400, val loss 2.3643\n",
            "iter 1500: loss 1.9332, time 11045.89ms, mfu 18.19%\n",
            "iter 1510: loss 2.0661, time 1968.87ms, mfu 18.36%\n",
            "iter 1520: loss 2.0736, time 1967.18ms, mfu 18.52%\n",
            "iter 1530: loss 2.0537, time 1969.62ms, mfu 18.66%\n",
            "iter 1540: loss 2.1093, time 1971.30ms, mfu 18.78%\n",
            "iter 1550: loss 2.1039, time 1975.09ms, mfu 18.89%\n",
            "iter 1560: loss 2.1023, time 1971.02ms, mfu 18.99%\n",
            "iter 1570: loss 2.0849, time 1978.02ms, mfu 19.08%\n",
            "iter 1580: loss 1.9852, time 1971.55ms, mfu 19.16%\n",
            "iter 1590: loss 1.9454, time 1972.86ms, mfu 19.23%\n",
            "iter 1600: loss 2.0350, time 1969.73ms, mfu 19.30%\n",
            "iter 1610: loss 2.0764, time 1974.35ms, mfu 19.35%\n",
            "iter 1620: loss 2.0567, time 1970.45ms, mfu 19.41%\n",
            "iter 1630: loss 1.9434, time 1971.70ms, mfu 19.46%\n",
            "iter 1640: loss 1.9748, time 1970.98ms, mfu 19.50%\n",
            "iter 1650: loss 1.9826, time 1975.11ms, mfu 19.54%\n",
            "iter 1660: loss 1.9701, time 1970.72ms, mfu 19.57%\n",
            "iter 1670: loss 2.1091, time 1968.54ms, mfu 19.61%\n",
            "iter 1680: loss 1.9433, time 1964.79ms, mfu 19.64%\n",
            "iter 1690: loss 1.8789, time 1973.81ms, mfu 19.67%\n",
            "iter 1700: loss 2.0952, time 1971.82ms, mfu 19.69%\n",
            "iter 1710: loss 1.9959, time 1970.92ms, mfu 19.71%\n",
            "iter 1720: loss 2.0114, time 1971.71ms, mfu 19.73%\n",
            "iter 1730: loss 1.9547, time 1968.89ms, mfu 19.75%\n",
            "iter 1740: loss 2.0173, time 1975.54ms, mfu 19.76%\n",
            "step 1750: train loss 1.8897, val loss 2.3604\n",
            "iter 1750: loss 2.0278, time 11028.73ms, mfu 18.14%\n",
            "iter 1760: loss 1.9087, time 1973.86ms, mfu 18.31%\n",
            "iter 1770: loss 2.1374, time 1970.06ms, mfu 18.47%\n",
            "iter 1780: loss 1.9212, time 1972.99ms, mfu 18.61%\n",
            "iter 1790: loss 1.8857, time 1970.67ms, mfu 18.74%\n",
            "iter 1800: loss 2.0154, time 1969.80ms, mfu 18.86%\n",
            "iter 1810: loss 1.9589, time 1972.44ms, mfu 18.96%\n",
            "iter 1820: loss 1.9221, time 1971.14ms, mfu 19.05%\n",
            "iter 1830: loss 2.0220, time 1975.87ms, mfu 19.13%\n",
            "iter 1840: loss 1.9556, time 1971.42ms, mfu 19.21%\n",
            "iter 1850: loss 2.0641, time 1978.95ms, mfu 19.27%\n",
            "iter 1860: loss 2.0454, time 1968.70ms, mfu 19.33%\n",
            "iter 1870: loss 2.0449, time 1979.70ms, mfu 19.38%\n",
            "iter 1880: loss 1.9539, time 1971.21ms, mfu 19.43%\n",
            "iter 1890: loss 1.9608, time 1974.79ms, mfu 19.48%\n",
            "iter 1900: loss 2.0721, time 1973.85ms, mfu 19.52%\n",
            "iter 1910: loss 2.0362, time 1973.30ms, mfu 19.55%\n",
            "iter 1920: loss 1.9928, time 1972.74ms, mfu 19.58%\n",
            "iter 1930: loss 1.9559, time 1969.97ms, mfu 19.62%\n",
            "iter 1940: loss 2.0007, time 1970.33ms, mfu 19.65%\n",
            "iter 1950: loss 2.0108, time 1967.37ms, mfu 19.67%\n",
            "iter 1960: loss 1.9892, time 1971.72ms, mfu 19.70%\n",
            "iter 1970: loss 2.0091, time 1968.72ms, mfu 19.72%\n",
            "iter 1980: loss 1.9944, time 1970.07ms, mfu 19.74%\n",
            "iter 1990: loss 1.9498, time 1971.62ms, mfu 19.75%\n",
            "step 2000: train loss 1.8553, val loss 2.3593\n",
            "iter 2000: loss 2.0548, time 11023.20ms, mfu 18.13%\n",
            "iter 2010: loss 1.9103, time 1969.14ms, mfu 18.31%\n",
            "iter 2020: loss 2.0200, time 1967.12ms, mfu 18.47%\n",
            "iter 2030: loss 1.9725, time 1970.70ms, mfu 18.62%\n",
            "iter 2040: loss 2.0184, time 1969.65ms, mfu 18.75%\n",
            "iter 2050: loss 1.8794, time 1964.49ms, mfu 18.87%\n",
            "iter 2060: loss 1.8660, time 1971.67ms, mfu 18.97%\n",
            "iter 2070: loss 1.9918, time 1969.44ms, mfu 19.06%\n",
            "iter 2080: loss 1.8683, time 1970.26ms, mfu 19.15%\n",
            "iter 2090: loss 1.8835, time 1972.40ms, mfu 19.22%\n",
            "iter 2100: loss 1.8935, time 1971.04ms, mfu 19.29%\n",
            "iter 2110: loss 1.9273, time 1970.29ms, mfu 19.35%\n",
            "iter 2120: loss 1.9188, time 1969.83ms, mfu 19.41%\n",
            "iter 2130: loss 1.8908, time 1981.07ms, mfu 19.45%\n",
            "iter 2140: loss 1.9886, time 1968.36ms, mfu 19.49%\n",
            "iter 2150: loss 1.9759, time 1979.71ms, mfu 19.53%\n",
            "iter 2160: loss 1.9207, time 1969.92ms, mfu 19.56%\n",
            "iter 2170: loss 1.9464, time 1977.32ms, mfu 19.59%\n",
            "iter 2180: loss 1.9523, time 1971.84ms, mfu 19.62%\n",
            "iter 2190: loss 2.0501, time 1975.71ms, mfu 19.64%\n",
            "iter 2200: loss 1.9883, time 1976.74ms, mfu 19.66%\n",
            "iter 2210: loss 1.9588, time 1979.37ms, mfu 19.68%\n",
            "iter 2220: loss 1.9910, time 1971.84ms, mfu 19.70%\n",
            "iter 2230: loss 2.0117, time 1970.60ms, mfu 19.72%\n",
            "iter 2240: loss 1.9251, time 1971.10ms, mfu 19.74%\n",
            "step 2250: train loss 1.8194, val loss 2.3593\n",
            "iter 2250: loss 1.9382, time 11049.32ms, mfu 18.12%\n",
            "iter 2260: loss 2.0097, time 1974.95ms, mfu 18.29%\n",
            "iter 2270: loss 1.9015, time 1969.57ms, mfu 18.45%\n",
            "iter 2280: loss 2.0353, time 1972.76ms, mfu 18.60%\n",
            "iter 2290: loss 2.0907, time 1971.44ms, mfu 18.73%\n",
            "iter 2300: loss 1.9826, time 1971.41ms, mfu 18.84%\n",
            "iter 2310: loss 1.9702, time 1971.64ms, mfu 18.95%\n",
            "iter 2320: loss 1.9028, time 1971.86ms, mfu 19.04%\n",
            "iter 2330: loss 1.9877, time 1969.35ms, mfu 19.13%\n",
            "iter 2340: loss 1.9030, time 1969.10ms, mfu 19.21%\n",
            "iter 2350: loss 1.9738, time 1971.84ms, mfu 19.28%\n",
            "iter 2360: loss 1.9861, time 1978.71ms, mfu 19.33%\n",
            "iter 2370: loss 1.9763, time 1974.62ms, mfu 19.38%\n",
            "iter 2380: loss 1.9718, time 1969.10ms, mfu 19.44%\n",
            "iter 2390: loss 2.0010, time 1975.32ms, mfu 19.48%\n",
            "iter 2400: loss 1.9885, time 1971.00ms, mfu 19.52%\n",
            "iter 2410: loss 1.8592, time 1976.54ms, mfu 19.55%\n",
            "iter 2420: loss 1.8489, time 1968.11ms, mfu 19.59%\n",
            "iter 2430: loss 1.9510, time 1970.27ms, mfu 19.62%\n",
            "iter 2440: loss 1.8349, time 1968.85ms, mfu 19.65%\n",
            "iter 2450: loss 1.9628, time 1976.83ms, mfu 19.67%\n",
            "iter 2460: loss 1.9802, time 1973.75ms, mfu 19.69%\n",
            "iter 2470: loss 1.9165, time 1972.26ms, mfu 19.71%\n",
            "iter 2480: loss 1.9163, time 1969.38ms, mfu 19.73%\n",
            "iter 2490: loss 1.8506, time 1970.82ms, mfu 19.75%\n",
            "step 2500: train loss 1.7900, val loss 2.3581\n",
            "iter 2500: loss 1.9721, time 11009.41ms, mfu 18.13%\n",
            "iter 2510: loss 1.9189, time 1970.89ms, mfu 18.31%\n",
            "iter 2520: loss 1.9405, time 1973.77ms, mfu 18.46%\n",
            "iter 2530: loss 1.9295, time 1969.78ms, mfu 18.61%\n",
            "iter 2540: loss 2.0388, time 1974.87ms, mfu 18.73%\n",
            "iter 2550: loss 1.9759, time 1971.04ms, mfu 18.85%\n",
            "iter 2560: loss 1.9139, time 1973.93ms, mfu 18.95%\n",
            "iter 2570: loss 1.9831, time 1969.84ms, mfu 19.05%\n",
            "iter 2580: loss 1.9296, time 1969.60ms, mfu 19.13%\n",
            "iter 2590: loss 1.8693, time 1969.46ms, mfu 19.21%\n",
            "iter 2600: loss 2.0230, time 1970.28ms, mfu 19.28%\n",
            "iter 2610: loss 1.9513, time 1974.14ms, mfu 19.34%\n",
            "iter 2620: loss 2.0086, time 1966.10ms, mfu 19.40%\n",
            "iter 2630: loss 1.9901, time 1974.57ms, mfu 19.45%\n",
            "iter 2640: loss 1.9702, time 1969.94ms, mfu 19.49%\n",
            "iter 2650: loss 1.8456, time 1974.76ms, mfu 19.53%\n",
            "iter 2660: loss 2.0108, time 1969.90ms, mfu 19.57%\n",
            "iter 2670: loss 1.9782, time 1975.37ms, mfu 19.60%\n",
            "iter 2680: loss 1.9316, time 1970.82ms, mfu 19.63%\n",
            "iter 2690: loss 1.8088, time 1974.96ms, mfu 19.65%\n",
            "iter 2700: loss 1.9006, time 1970.05ms, mfu 19.68%\n",
            "iter 2710: loss 1.8770, time 1970.89ms, mfu 19.70%\n",
            "iter 2720: loss 1.9817, time 1970.65ms, mfu 19.72%\n",
            "iter 2730: loss 1.9845, time 1975.30ms, mfu 19.73%\n",
            "iter 2740: loss 1.9273, time 1972.87ms, mfu 19.75%\n",
            "step 2750: train loss 1.7709, val loss 2.3610\n",
            "iter 2750: loss 1.8611, time 11020.18ms, mfu 18.13%\n",
            "iter 2760: loss 1.8928, time 1975.95ms, mfu 18.30%\n",
            "iter 2770: loss 1.8590, time 1970.63ms, mfu 18.46%\n",
            "iter 2780: loss 1.6983, time 1975.78ms, mfu 18.60%\n",
            "iter 2790: loss 1.8395, time 1974.49ms, mfu 18.73%\n",
            "iter 2800: loss 1.9855, time 1973.11ms, mfu 18.84%\n",
            "iter 2810: loss 1.9578, time 1969.46ms, mfu 18.95%\n",
            "iter 2820: loss 1.9184, time 1974.17ms, mfu 19.04%\n",
            "iter 2830: loss 1.9100, time 1970.30ms, mfu 19.13%\n",
            "iter 2840: loss 1.9046, time 1970.40ms, mfu 19.20%\n",
            "iter 2850: loss 1.8811, time 1974.79ms, mfu 19.27%\n",
            "iter 2860: loss 1.9886, time 1968.54ms, mfu 19.33%\n",
            "iter 2870: loss 1.9967, time 1971.69ms, mfu 19.39%\n",
            "iter 2880: loss 1.8809, time 1965.71ms, mfu 19.45%\n",
            "iter 2890: loss 1.8736, time 1977.31ms, mfu 19.49%\n",
            "iter 2900: loss 2.0296, time 1968.98ms, mfu 19.53%\n",
            "iter 2910: loss 2.0113, time 1975.71ms, mfu 19.56%\n",
            "iter 2920: loss 1.9586, time 1971.45ms, mfu 19.59%\n",
            "iter 2930: loss 1.9858, time 1977.99ms, mfu 19.62%\n",
            "iter 2940: loss 1.8476, time 1970.93ms, mfu 19.65%\n",
            "iter 2950: loss 1.8891, time 1972.26ms, mfu 19.67%\n",
            "iter 2960: loss 1.9313, time 1969.75ms, mfu 19.69%\n",
            "iter 2970: loss 1.9493, time 1972.49ms, mfu 19.71%\n",
            "iter 2980: loss 1.7752, time 1973.20ms, mfu 19.73%\n",
            "iter 2990: loss 1.8596, time 1975.60ms, mfu 19.74%\n",
            "step 3000: train loss 1.7465, val loss 2.3731\n",
            "iter 3000: loss 1.9804, time 11040.49ms, mfu 18.12%\n",
            "iter 3010: loss 1.8104, time 1972.27ms, mfu 18.30%\n",
            "iter 3020: loss 1.8592, time 1975.87ms, mfu 18.45%\n",
            "iter 3030: loss 1.8977, time 1971.63ms, mfu 18.60%\n",
            "iter 3040: loss 1.8775, time 1973.24ms, mfu 18.73%\n",
            "iter 3050: loss 1.9194, time 1973.15ms, mfu 18.84%\n",
            "iter 3060: loss 1.7639, time 1974.19ms, mfu 18.94%\n",
            "iter 3070: loss 1.8310, time 1968.49ms, mfu 19.04%\n",
            "iter 3080: loss 1.9931, time 1975.53ms, mfu 19.12%\n",
            "iter 3090: loss 1.9672, time 1970.28ms, mfu 19.20%\n",
            "iter 3100: loss 1.8812, time 1967.77ms, mfu 19.27%\n",
            "iter 3110: loss 1.8204, time 1968.47ms, mfu 19.34%\n",
            "iter 3120: loss 1.8227, time 1969.56ms, mfu 19.40%\n",
            "iter 3130: loss 1.8522, time 1971.41ms, mfu 19.45%\n",
            "iter 3140: loss 1.9153, time 1970.42ms, mfu 19.49%\n",
            "iter 3150: loss 1.9211, time 1980.10ms, mfu 19.52%\n",
            "iter 3160: loss 1.9462, time 1971.93ms, mfu 19.56%\n",
            "iter 3170: loss 1.9453, time 1978.01ms, mfu 19.59%\n",
            "iter 3180: loss 1.8095, time 1969.04ms, mfu 19.62%\n",
            "iter 3190: loss 1.8421, time 1976.77ms, mfu 19.64%\n",
            "iter 3200: loss 1.9548, time 1971.45ms, mfu 19.67%\n",
            "iter 3210: loss 1.9039, time 1975.72ms, mfu 19.68%\n",
            "iter 3220: loss 1.8401, time 1970.16ms, mfu 19.71%\n",
            "iter 3230: loss 1.8497, time 1969.28ms, mfu 19.73%\n",
            "iter 3240: loss 1.7923, time 1970.04ms, mfu 19.75%\n",
            "step 3250: train loss 1.7358, val loss 2.3813\n",
            "iter 3250: loss 1.8513, time 11015.97ms, mfu 18.13%\n",
            "iter 3260: loss 1.9105, time 1970.40ms, mfu 18.30%\n",
            "iter 3270: loss 1.8485, time 1968.13ms, mfu 18.47%\n",
            "iter 3280: loss 1.8025, time 1972.91ms, mfu 18.61%\n",
            "iter 3290: loss 1.8497, time 1968.88ms, mfu 18.74%\n",
            "iter 3300: loss 1.9041, time 1974.05ms, mfu 18.85%\n",
            "iter 3310: loss 1.9714, time 1971.49ms, mfu 18.96%\n",
            "iter 3320: loss 1.7661, time 1972.65ms, mfu 19.05%\n",
            "iter 3330: loss 1.7687, time 1971.97ms, mfu 19.13%\n",
            "iter 3340: loss 1.9091, time 1971.72ms, mfu 19.21%\n",
            "iter 3350: loss 1.9141, time 1971.75ms, mfu 19.28%\n",
            "iter 3360: loss 1.7196, time 1966.30ms, mfu 19.34%\n",
            "iter 3370: loss 2.0055, time 1969.99ms, mfu 19.40%\n",
            "iter 3380: loss 1.9564, time 1969.63ms, mfu 19.45%\n",
            "iter 3390: loss 1.9137, time 1972.06ms, mfu 19.49%\n",
            "iter 3400: loss 1.8054, time 1971.74ms, mfu 19.53%\n",
            "iter 3410: loss 1.9432, time 1973.57ms, mfu 19.57%\n",
            "iter 3420: loss 1.9476, time 1971.27ms, mfu 19.60%\n",
            "iter 3430: loss 1.9192, time 1974.47ms, mfu 19.63%\n",
            "iter 3440: loss 1.8867, time 1967.12ms, mfu 19.66%\n",
            "iter 3450: loss 1.9041, time 1977.53ms, mfu 19.68%\n",
            "iter 3460: loss 1.8884, time 1973.59ms, mfu 19.69%\n",
            "iter 3470: loss 1.8270, time 1982.71ms, mfu 19.70%\n",
            "iter 3480: loss 1.9357, time 1973.55ms, mfu 19.72%\n",
            "iter 3490: loss 1.8495, time 1975.93ms, mfu 19.73%\n",
            "step 3500: train loss 1.7275, val loss 2.3657\n",
            "iter 3500: loss 1.9554, time 11026.65ms, mfu 18.12%\n",
            "iter 3510: loss 1.7595, time 1969.58ms, mfu 18.30%\n",
            "iter 3520: loss 1.8698, time 1976.03ms, mfu 18.45%\n",
            "iter 3530: loss 1.8410, time 1970.89ms, mfu 18.60%\n",
            "iter 3540: loss 1.8661, time 1973.67ms, mfu 18.72%\n",
            "iter 3550: loss 1.9066, time 1970.31ms, mfu 18.84%\n",
            "iter 3560: loss 1.8883, time 1972.69ms, mfu 18.94%\n",
            "iter 3570: loss 1.8283, time 1972.13ms, mfu 19.04%\n",
            "iter 3580: loss 1.8890, time 1974.49ms, mfu 19.12%\n",
            "iter 3590: loss 1.9005, time 1972.67ms, mfu 19.20%\n",
            "iter 3600: loss 1.9522, time 1972.95ms, mfu 19.27%\n",
            "iter 3610: loss 1.7132, time 1971.41ms, mfu 19.33%\n",
            "iter 3620: loss 1.9542, time 1971.29ms, mfu 19.38%\n",
            "iter 3630: loss 1.8415, time 1970.17ms, mfu 19.44%\n",
            "iter 3640: loss 1.8394, time 1969.95ms, mfu 19.48%\n",
            "iter 3650: loss 1.9235, time 1973.01ms, mfu 19.52%\n",
            "iter 3660: loss 1.9148, time 1976.98ms, mfu 19.55%\n",
            "iter 3670: loss 1.8522, time 1975.86ms, mfu 19.58%\n",
            "iter 3680: loss 1.9017, time 1972.31ms, mfu 19.61%\n",
            "iter 3690: loss 1.9211, time 1971.40ms, mfu 19.64%\n",
            "iter 3700: loss 1.8501, time 1973.75ms, mfu 19.67%\n",
            "iter 3710: loss 1.8499, time 1975.45ms, mfu 19.68%\n",
            "iter 3720: loss 2.0272, time 1970.96ms, mfu 19.71%\n",
            "iter 3730: loss 1.8216, time 1975.18ms, mfu 19.72%\n",
            "iter 3740: loss 1.8895, time 1967.39ms, mfu 19.74%\n",
            "step 3750: train loss 1.7070, val loss 2.3759\n",
            "iter 3750: loss 1.9812, time 11031.73ms, mfu 18.12%\n",
            "iter 3760: loss 1.8934, time 1977.94ms, mfu 18.29%\n",
            "iter 3770: loss 1.9112, time 1972.44ms, mfu 18.45%\n",
            "iter 3780: loss 1.9275, time 1981.93ms, mfu 18.59%\n",
            "iter 3790: loss 1.8268, time 1969.77ms, mfu 18.72%\n",
            "iter 3800: loss 1.9024, time 1979.69ms, mfu 18.83%\n",
            "iter 3810: loss 1.9047, time 1971.45ms, mfu 18.93%\n",
            "iter 3820: loss 1.8730, time 1976.05ms, mfu 19.03%\n",
            "iter 3830: loss 1.8828, time 1970.28ms, mfu 19.11%\n",
            "iter 3840: loss 1.9347, time 1973.63ms, mfu 19.19%\n",
            "iter 3850: loss 1.7813, time 1971.96ms, mfu 19.26%\n",
            "iter 3860: loss 1.9396, time 1973.55ms, mfu 19.32%\n",
            "iter 3870: loss 1.8195, time 1972.69ms, mfu 19.38%\n",
            "iter 3880: loss 1.8155, time 1970.91ms, mfu 19.43%\n",
            "iter 3890: loss 1.8302, time 1971.62ms, mfu 19.47%\n",
            "iter 3900: loss 1.8574, time 1969.99ms, mfu 19.52%\n",
            "iter 3910: loss 1.8559, time 1968.93ms, mfu 19.56%\n",
            "iter 3920: loss 1.9233, time 1969.72ms, mfu 19.59%\n",
            "iter 3930: loss 1.8588, time 1969.36ms, mfu 19.63%\n",
            "iter 3940: loss 1.8387, time 1971.65ms, mfu 19.65%\n",
            "iter 3950: loss 1.8008, time 1970.86ms, mfu 19.68%\n",
            "iter 3960: loss 1.8080, time 1971.96ms, mfu 19.70%\n",
            "iter 3970: loss 1.8421, time 1973.11ms, mfu 19.72%\n",
            "iter 3980: loss 1.8824, time 1970.76ms, mfu 19.73%\n",
            "iter 3990: loss 1.9228, time 1979.46ms, mfu 19.74%\n",
            "step 4000: train loss 1.6947, val loss 2.3779\n",
            "iter 4000: loss 1.8443, time 11049.38ms, mfu 18.12%\n",
            "iter 4010: loss 1.9211, time 1971.89ms, mfu 18.30%\n",
            "iter 4020: loss 1.8166, time 1970.27ms, mfu 18.46%\n",
            "iter 4030: loss 1.8475, time 1970.99ms, mfu 18.60%\n",
            "iter 4040: loss 1.7367, time 1974.34ms, mfu 18.73%\n",
            "iter 4050: loss 1.8048, time 1966.07ms, mfu 18.85%\n",
            "iter 4060: loss 1.8282, time 1974.99ms, mfu 18.95%\n",
            "iter 4070: loss 1.8525, time 1968.76ms, mfu 19.05%\n",
            "iter 4080: loss 1.7257, time 1975.39ms, mfu 19.13%\n",
            "iter 4090: loss 1.8299, time 1968.45ms, mfu 19.21%\n",
            "iter 4100: loss 1.8472, time 1973.37ms, mfu 19.28%\n",
            "iter 4110: loss 1.8787, time 1968.01ms, mfu 19.34%\n",
            "iter 4120: loss 1.7529, time 1971.23ms, mfu 19.40%\n",
            "iter 4130: loss 1.8864, time 1961.90ms, mfu 19.46%\n",
            "iter 4140: loss 1.8048, time 1971.23ms, mfu 19.50%\n",
            "iter 4150: loss 1.8541, time 1970.86ms, mfu 19.54%\n",
            "iter 4160: loss 1.8673, time 1971.25ms, mfu 19.58%\n",
            "iter 4170: loss 1.8778, time 1974.13ms, mfu 19.60%\n",
            "iter 4180: loss 1.8744, time 1970.69ms, mfu 19.63%\n",
            "iter 4190: loss 1.8593, time 1974.40ms, mfu 19.66%\n",
            "iter 4200: loss 1.8838, time 1968.57ms, mfu 19.68%\n",
            "iter 4210: loss 1.8846, time 1970.25ms, mfu 19.71%\n",
            "iter 4220: loss 1.8929, time 1970.74ms, mfu 19.73%\n",
            "iter 4230: loss 1.7763, time 1975.40ms, mfu 19.74%\n",
            "iter 4240: loss 1.9531, time 1970.58ms, mfu 19.75%\n",
            "step 4250: train loss 1.6869, val loss 2.3832\n",
            "iter 4250: loss 1.8602, time 11055.58ms, mfu 18.13%\n",
            "iter 4260: loss 1.7630, time 1971.23ms, mfu 18.31%\n",
            "iter 4270: loss 1.7773, time 1969.44ms, mfu 18.47%\n",
            "iter 4280: loss 1.8776, time 1968.20ms, mfu 18.62%\n",
            "iter 4290: loss 1.7612, time 1974.37ms, mfu 18.74%\n",
            "iter 4300: loss 1.8680, time 1974.43ms, mfu 18.85%\n",
            "iter 4310: loss 1.8221, time 1969.41ms, mfu 18.96%\n",
            "iter 4320: loss 1.8898, time 1974.55ms, mfu 19.05%\n",
            "iter 4330: loss 1.9121, time 1974.03ms, mfu 19.13%\n",
            "iter 4340: loss 1.8879, time 1973.15ms, mfu 19.21%\n",
            "iter 4350: loss 2.0093, time 1970.99ms, mfu 19.27%\n",
            "iter 4360: loss 1.8448, time 1976.51ms, mfu 19.33%\n",
            "iter 4370: loss 1.8043, time 1970.92ms, mfu 19.39%\n",
            "iter 4380: loss 1.7313, time 1975.02ms, mfu 19.44%\n",
            "iter 4390: loss 1.9062, time 1970.61ms, mfu 19.48%\n",
            "iter 4400: loss 1.9407, time 1972.75ms, mfu 19.52%\n",
            "iter 4410: loss 1.8771, time 1969.28ms, mfu 19.56%\n",
            "iter 4420: loss 1.7711, time 1974.32ms, mfu 19.59%\n",
            "iter 4430: loss 1.8323, time 1970.98ms, mfu 19.62%\n",
            "iter 4440: loss 1.8188, time 1969.02ms, mfu 19.65%\n",
            "iter 4450: loss 1.7987, time 1977.67ms, mfu 19.67%\n",
            "iter 4460: loss 1.7753, time 1971.64ms, mfu 19.69%\n",
            "iter 4470: loss 1.8778, time 1973.95ms, mfu 19.71%\n",
            "iter 4480: loss 1.8854, time 1971.34ms, mfu 19.73%\n",
            "iter 4490: loss 1.7730, time 1974.71ms, mfu 19.74%\n",
            "step 4500: train loss 1.6825, val loss 2.3717\n",
            "iter 4500: loss 1.7744, time 11046.15ms, mfu 18.12%\n",
            "iter 4510: loss 1.8045, time 1970.06ms, mfu 18.30%\n",
            "iter 4520: loss 1.8499, time 1971.24ms, mfu 18.46%\n",
            "iter 4530: loss 1.7769, time 1973.23ms, mfu 18.60%\n",
            "iter 4540: loss 1.9927, time 1969.97ms, mfu 18.73%\n",
            "iter 4550: loss 1.8683, time 1970.96ms, mfu 18.85%\n",
            "iter 4560: loss 1.8078, time 1974.93ms, mfu 18.95%\n",
            "iter 4570: loss 1.7493, time 1969.96ms, mfu 19.05%\n",
            "iter 4580: loss 1.8947, time 1976.46ms, mfu 19.13%\n",
            "iter 4590: loss 1.7695, time 1968.86ms, mfu 19.20%\n",
            "iter 4600: loss 1.8815, time 1974.87ms, mfu 19.27%\n",
            "iter 4610: loss 1.8988, time 1969.83ms, mfu 19.33%\n",
            "iter 4620: loss 1.8114, time 1976.81ms, mfu 19.38%\n",
            "iter 4630: loss 1.8785, time 1973.43ms, mfu 19.43%\n",
            "iter 4640: loss 1.7811, time 1971.92ms, mfu 19.48%\n",
            "iter 4650: loss 1.8225, time 1971.84ms, mfu 19.52%\n",
            "iter 4660: loss 1.7467, time 1971.61ms, mfu 19.56%\n",
            "iter 4670: loss 1.8227, time 1971.23ms, mfu 19.59%\n",
            "iter 4680: loss 1.9302, time 1974.27ms, mfu 19.62%\n",
            "iter 4690: loss 1.8097, time 1970.08ms, mfu 19.65%\n",
            "iter 4700: loss 1.7938, time 1965.40ms, mfu 19.68%\n",
            "iter 4710: loss 1.8656, time 1969.18ms, mfu 19.70%\n",
            "iter 4720: loss 1.7912, time 1973.36ms, mfu 19.72%\n",
            "iter 4730: loss 1.7604, time 1969.44ms, mfu 19.74%\n",
            "iter 4740: loss 1.8312, time 1970.22ms, mfu 19.76%\n",
            "step 4750: train loss 1.6715, val loss 2.3895\n",
            "iter 4750: loss 1.8459, time 11030.56ms, mfu 18.14%\n",
            "iter 4760: loss 1.7945, time 1968.69ms, mfu 18.31%\n",
            "iter 4770: loss 1.8390, time 1975.47ms, mfu 18.47%\n",
            "iter 4780: loss 1.8320, time 1971.57ms, mfu 18.61%\n",
            "iter 4790: loss 1.7705, time 1968.16ms, mfu 18.74%\n",
            "iter 4800: loss 1.8729, time 1966.56ms, mfu 18.86%\n",
            "iter 4810: loss 1.7806, time 1969.88ms, mfu 18.97%\n",
            "iter 4820: loss 1.8862, time 1978.27ms, mfu 19.05%\n",
            "iter 4830: loss 1.7860, time 1980.55ms, mfu 19.13%\n",
            "iter 4840: loss 1.9064, time 1973.19ms, mfu 19.20%\n",
            "iter 4850: loss 1.8144, time 1974.04ms, mfu 19.27%\n",
            "iter 4860: loss 1.7542, time 1976.55ms, mfu 19.33%\n",
            "iter 4870: loss 1.7881, time 1969.72ms, mfu 19.38%\n",
            "iter 4880: loss 1.8991, time 1979.51ms, mfu 19.43%\n",
            "iter 4890: loss 1.8945, time 1973.47ms, mfu 19.47%\n",
            "iter 4900: loss 1.8039, time 1970.19ms, mfu 19.52%\n",
            "iter 4910: loss 1.9488, time 1970.61ms, mfu 19.55%\n",
            "iter 4920: loss 1.7759, time 1978.16ms, mfu 19.58%\n",
            "iter 4930: loss 1.7899, time 1971.53ms, mfu 19.61%\n",
            "iter 4940: loss 1.8344, time 1975.47ms, mfu 19.64%\n",
            "iter 4950: loss 1.9142, time 1967.18ms, mfu 19.67%\n",
            "iter 4960: loss 1.7201, time 1973.68ms, mfu 19.69%\n",
            "iter 4970: loss 1.8108, time 1975.00ms, mfu 19.70%\n",
            "iter 4980: loss 1.7547, time 1973.26ms, mfu 19.72%\n",
            "iter 4990: loss 1.8195, time 1972.32ms, mfu 19.74%\n",
            "step 5000: train loss 1.6726, val loss 2.3793\n",
            "iter 5000: loss 1.7086, time 11047.98ms, mfu 18.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sample.py :\n",
        " - LOVE를 질의하여 노래가사 생성 \n",
        " - number of parameters: 29.94M\n",
        " - GPT-2 encodings"
      ],
      "metadata": {
        "id": "bYy88jFA0Ihf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run sample.py --out_dir=out-songwriter --start=\"LOVE\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzONthFh0H5E",
        "outputId": "d52c0eb1-b140-45e3-86a6-979a855c4445"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-songwriter\n",
            "Overriding: start = LOVE\n",
            "number of parameters: 29.94M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "LOVE ME !  \n",
            "  \n",
            "  \n",
            "And it's beautiful  \n",
            "Forget about your face  \n",
            "I'll fill the air  \n",
            "I can't believe it  \n",
            "That I'm ok  \n",
            "I will be ok  \n",
            "And I will be ok  \n",
            "And I will be ok  \n",
            "  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "  \n",
            "So come on  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok  \n",
            "I'm ok\n",
            "\n",
            "\n",
            "I ain't just for you  \n",
            "They don't come without me  \n",
            "  \n",
            "Nobody lives in the world for me  \n",
            "Nobody lives in the world for me  \n",
            "Nobody lives in the world for me  \n",
            "Nobody lives in the world for me  \n",
            "  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you  \n",
            "It's not like you\n",
            "\n",
            "\n",
            "Why must it be that way  \n",
            "To be so lost, so alone  \n",
            "Inside my mind  \n",
            "So I'm afraid that I've got a ticket to hold on  \n",
            "---------------\n",
            "LOVE TRAMLOVE  \n",
            "  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation  \n",
            "Temptation\n",
            "\n",
            "\n",
            "I'm a man and my love is all a man\n",
            "\n",
            "\n",
            "You are the light that shines  \n",
            "When it's time to arise  \n",
            "From the back of my mind  \n",
            "You are the one  \n",
            "You are the one  \n",
            "You are the one  \n",
            "  \n",
            "You are the light that shines  \n",
            "When it's time to arise  \n",
            "From the back of my mind  \n",
            "You are the light that shines  \n",
            "When it's time to arise  \n",
            " from the back of my mind  \n",
            "You are the light that shines  \n",
            "When it's time to arise  \n",
            "From the back of my mind  \n",
            "  \n",
            "I'm a man and my love is all a man  \n",
            "You are the light that shines  \n",
            "When it's time to arise  \n",
            "From the back of my mind  \n",
            "You are the light that shines  \n",
            "When it's time to arise  \n",
            "From the back of my mind  \n",
            "You are the light that shines  \n",
            "When it's time to arise  \n",
            "From the back of my mind\n",
            "\n",
            "\n",
            "I've been living in a lie  \n",
            "But I've seen that someone's eyes  \n",
            "Watching people passing by  \n",
            "I'm standing here alone  \n",
            "I'm still alive  \n",
            "  \n",
            "And if I make it last  \n",
            "I'd give it all to myself  \n",
            "I'd give it all to myself  \n",
            "I'd give it all to myself  \n",
            "I'd give it all to myself  \n",
            "I'd give it all to myself  \n",
            "I'd give it all, I'd give it all to myself  \n",
            "I\n",
            "---------------\n",
            "LOVE YOUR OWN NAME  \n",
            "  \n",
            "I'm no longer alone,  \n",
            "No more dyin',  \n",
            "No more dyin',  \n",
            "No more dyin',  \n",
            "No more dyin'  \n",
            "  \n",
            "I want to feel free,  \n",
            "I want to feel free,  \n",
            "I want to feel free  \n",
            "I want to feel free.  \n",
            "  \n",
            "I want to feel free,  \n",
            "I want to feel free,  \n",
            "I want to feel free,  \n",
            "I want to feel free.  \n",
            "  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,  \n",
            "I got a little bit of bones,\n",
            "\n",
            "\n",
            "A little deeper, no, no, hey now  \n",
            "A little deeper, no, no, hey now  \n",
            "A little deeper, no, hey, hey now  \n",
            "A little deeper, yeah, hey now  \n",
            "A little deeper, yeah, yeah  \n",
            "A little deeper, yeah, hey now  \n",
            "A little deeper, yeah, yeah, yeah, yeah  \n",
            "A little deeper, yeah, yeah  \n",
            "A little deeper, yeah, hey now  \n",
            "A little deeper, yeah yeah, yeah  \n",
            "A little deeper, yeah, yeah, yeah  \n",
            "A little deeper, yeah, yeah, yeah  \n",
            "A little deeper, yeah, yeah, yeah  \n",
            "A little deeper, yeah, yeah, yeah  \n",
            "A little more,\n",
            "---------------\n",
            "LOVE YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP YOUR HELP\n",
            "---------------\n",
            "LOVE ME, BEFORE YOU TRHER  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK I  \n",
            "NO WHO THINK YOU  \n",
            "NO WHO THINK YOU\n",
            "\n",
            "\n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you're looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you're looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now,  \n",
            "If you are looking real closely now, \n",
            "---------------\n",
            "LOVE YOUR PRESENCE LOVE  \n",
            "You're just another boy.  \n",
            "You're just another boy.  \n",
            "You're just another boy.\n",
            "\n",
            "\n",
            "I'm not a great big man  \n",
            "I don't know what to do  \n",
            "But I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do  \n",
            "I don't know what to do\n",
            "\n",
            "\n",
            "You're always lonely  \n",
            "You've been broken and I'll be lost  \n",
            "And I know that I've been wrong  \n",
            "I've been away too long  \n",
            "I've been away too long  \n",
            "  \n",
            "Lonely, waiting for you  \n",
            "Touching your eyes  \n",
            "And I know that I've been wrong  \n",
            "I've been away too long  \n",
            "  \n",
            "Now I'm trying, I'm trying  \n",
            "To follow you  \n",
            "I'm trying, I'm trying  \n",
            "To follow you  \n",
            "  \n",
            "I know that I've been wrong  \n",
            "I've been away too long  \n",
            "I've been away too long  \n",
            "  \n",
            "I've been away too long  \n",
            "I've been away too long \n",
            "---------------\n",
            "LOVE COME INTO THIS HOUSE AWAY  \n",
            "  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school  \n",
            "Back to school\n",
            "\n",
            "\n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't count so much  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice  \n",
            "Don't look so nice\n",
            "\n",
            "\n",
            "I got a problem  \n",
            "The devil is a bitch  \n",
            "'Til everybody knows  \n",
            "I ain't talking bout  \n",
            "What you say  \n",
            "Ain't no more  \n",
            "All it's really on your mind  \n",
            "  \n",
            "I got a problem  \n",
            "The devil is a bitch  \n",
            "'Til everybody knows  \n",
            "And I ain't talking bout  \n",
            "What you say  \n",
            "Ain't no more  \n",
            "All it's really on your mind  \n",
            "  \n",
            "I think you like  \n",
            "I think you like  \n",
            "I think you like\n",
            "---------------\n",
            "LOVE ME, BEFORE YOU KNOW  \n",
            "LOVE ME, BEFORE YOU KNOW  \n",
            "LOVE ME, BEFORE YOU KNOW  \n",
            "LOVE ME  \n",
            "LOVE ME, BEFORE YOU KNOW  \n",
            "LOVE ME,LOVE ME\n",
            "\n",
            "\n",
            "Walking in the alley house  \n",
            "In a room with no suitcase  \n",
            "Lose myself in your room  \n",
            "And a mind that's filled with lies  \n",
            "Hear my voice, hear my voice  \n",
            "  \n",
            "When I was lost I'd lost my mind  \n",
            "Like a man who would rather find the day  \n",
            "No matter what you do  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "But I'm all alone  \n",
            "  \n",
            "I've been staring at you  \n",
            "And your eyes  \n",
            "I've seen you every time  \n",
            "You're all alone  \n",
            "But you never wanna be alone  \n",
            "  \n",
            "The way your gonna come out tonight  \n",
            "I don't wanna go on get with you  \n",
            "I know I'm all alone  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone  \n",
            "I guess I'm all alone\n",
            "\n",
            "\n",
            "When you were seventeen,  \n",
            "I was seventeen,  \n",
            "So many times you had me fooled.  \n",
            "  \n",
            "Well, I was seventeen,  \n",
            "But in my mind I was five  \n",
            "That was so alone,  \n",
            "All my friends were so lonely.  \n",
            "  \n",
            "You would be my first only one,  \n",
            "Who could change your mind.  \n",
            "  \n",
            "You would be my first only one,  \n",
            "Who could change your mind,  \n",
            "---------------\n",
            "LOVE SHEH  \n",
            "This is the end, it's the end  \n",
            "Check out the door and I'll be there.  \n",
            "  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.  \n",
            "You're a cold and lonely stranger,  \n",
            "When it's cold outside, you can't escape the world.\n",
            "\n",
            "\n",
            "In the park  \n",
            "In the dark with the stars in your eyes  \n",
            "In the dark with the stars in your eyes  \n",
            "I'll be there at night  \n",
            "  \n",
            "I'll be there at\n",
            "---------------\n",
            "LOVE TRUE  \n",
            "My name is QUEOW IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE  \n",
            "YOUR DAY IN LOVE\n",
            "\n",
            "\n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "(You don't want to be afraid)  \n",
            "  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "(You don't want to be afraid)  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "You don't want to be afraid  \n",
            "(You don't want to be afraid)\n",
            "\n",
            "\n",
            "My heart is moving fast  \n",
            "For you to break  \n",
            "It's in my heart  \n",
            "I'm in a different way  \n",
            "  \n",
            "I'm here to find you  \n",
            "My mind is moving fast \n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}